#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤ì‹œê°„ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸
ë°ì´í„° ìˆ˜ì§‘, ì§‘ê³„, ìºì‹±, ë°°í¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì¤‘ì•™ íŒŒì´í”„ë¼ì¸
"""

import json
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
import schedule
from pathlib import Path

from .performance_metrics import PerformanceMetricsCalculator, RealTimeMetrics
from ..database.schema import db_manager


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    update_interval_seconds: int = 300  # 5ë¶„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
    cache_duration_minutes: int = 5     # 5ë¶„ ìºì‹œ
    enable_real_time_updates: bool = True
    export_to_file: bool = True
    export_directory: str = "data/dashboard_cache"
    max_cache_files: int = 100


class DashboardDataPipeline:
    """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Optional[PipelineConfig] = None):
        self.config = config or PipelineConfig()
        self.metrics_calculator = PerformanceMetricsCalculator()
        
        # ë‚´ë¶€ ìƒíƒœ
        self._is_running = False
        self._update_thread = None
        self._subscribers: List[Callable[[RealTimeMetrics], None]] = []
        self._latest_metrics: Optional[RealTimeMetrics] = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.cache_dir = Path(self.config.export_directory)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“Š ëŒ€ì‹œë³´ë“œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì—…ë°ì´íŠ¸ ì£¼ê¸°: {self.config.update_interval_seconds}ì´ˆ")
        print(f"   - ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
    
    def start(self):
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        if self._is_running:
            print("âš ï¸  íŒŒì´í”„ë¼ì¸ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        print("ğŸš€ ëŒ€ì‹œë³´ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        self._is_running = True
        
        # ì´ˆê¸° ë©”íŠ¸ë¦­ ê³„ì‚°
        self._update_metrics()
        
        if self.config.enable_real_time_updates:
            # ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
            self._update_thread = threading.Thread(target=self._run_update_loop, daemon=True)
            self._update_thread.start()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
            schedule.every(self.config.update_interval_seconds).seconds.do(self._update_metrics)
            
        print("âœ… íŒŒì´í”„ë¼ì¸ ì‹œì‘ë¨")
    
    def stop(self):
        """íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        if not self._is_running:
            return
        
        print("ğŸ›‘ ëŒ€ì‹œë³´ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¤‘ì§€")
        self._is_running = False
        
        if self._update_thread:
            self._update_thread.join(timeout=5)
        
        schedule.clear()
        print("âœ… íŒŒì´í”„ë¼ì¸ ì¤‘ì§€ë¨")
    
    def _run_update_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ë£¨í”„"""
        while self._is_running:
            try:
                schedule.run_pending()
                time.sleep(1)
            except Exception as e:
                print(f"âŒ ì—…ë°ì´íŠ¸ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(5)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 5ì´ˆ ëŒ€ê¸°
    
    def _update_metrics(self):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            start_time = time.time()
            
            # ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self.metrics_calculator.get_real_time_metrics(refresh_cache=True)
            self._latest_metrics = metrics
            
            # íŒŒì¼ë¡œ export (ì„¤ì •ëœ ê²½ìš°)
            if self.config.export_to_file:
                self._export_to_file(metrics)
            
            # êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼
            self._notify_subscribers(metrics)
            
            duration = time.time() - start_time
            print(f"ğŸ“Š ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({duration:.2f}ì´ˆ)")
            
        except Exception as e:
            print(f"âŒ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _export_to_file(self, metrics: RealTimeMetrics):
        """ë©”íŠ¸ë¦­ì„ íŒŒì¼ë¡œ export"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"dashboard_metrics_{timestamp}.json"
            filepath = self.cache_dir / filename
            
            # JSON íŒŒì¼ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(metrics.to_dict(), f, ensure_ascii=False, indent=2)
            
            # ìµœì‹  íŒŒì¼ì— ëŒ€í•œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
            latest_filepath = self.cache_dir / "latest_metrics.json"
            if latest_filepath.exists():
                latest_filepath.unlink()
            
            with open(latest_filepath, 'w', encoding='utf-8') as f:
                json.dump(metrics.to_dict(), f, ensure_ascii=False, indent=2)
            
            # ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬
            self._cleanup_old_cache_files()
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ export ì‹¤íŒ¨: {e}")
    
    def _cleanup_old_cache_files(self):
        """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            cache_files = list(self.cache_dir.glob("dashboard_metrics_*.json"))
            cache_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
            if len(cache_files) > self.config.max_cache_files:
                for old_file in cache_files[self.config.max_cache_files:]:
                    old_file.unlink()
                    
        except Exception as e:
            print(f"âŒ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def subscribe(self, callback: Callable[[RealTimeMetrics], None]):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ êµ¬ë…"""
        self._subscribers.append(callback)
        print(f"ğŸ“¡ êµ¬ë…ì ì¶”ê°€ë¨ (ì´ {len(self._subscribers)}ëª…)")
        
        # ìµœì‹  ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì „ì†¡
        if self._latest_metrics:
            try:
                callback(self._latest_metrics)
            except Exception as e:
                print(f"âŒ êµ¬ë…ì ì•Œë¦¼ ì‹¤íŒ¨: {e}")
    
    def unsubscribe(self, callback: Callable[[RealTimeMetrics], None]):
        """êµ¬ë… í•´ì œ"""
        if callback in self._subscribers:
            self._subscribers.remove(callback)
            print(f"ğŸ“¡ êµ¬ë…ì ì œê±°ë¨ (ì´ {len(self._subscribers)}ëª…)")
    
    def _notify_subscribers(self, metrics: RealTimeMetrics):
        """êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼"""
        for callback in self._subscribers[:]:  # ë³µì‚¬ë³¸ìœ¼ë¡œ ìˆœíšŒ
            try:
                callback(metrics)
            except Exception as e:
                print(f"âŒ êµ¬ë…ì ì•Œë¦¼ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ êµ¬ë…ì ì œê±°
                self._subscribers.remove(callback)
    
    def get_latest_metrics(self) -> Optional[RealTimeMetrics]:
        """ìµœì‹  ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self._latest_metrics
    
    def force_update(self):
        """ê°•ì œ ì—…ë°ì´íŠ¸"""
        print("ğŸ”„ ê°•ì œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤í–‰")
        self._update_metrics()
    
    def get_cached_metrics(self, hours_back: int = 24) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        try:
            cached_metrics = []
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            
            cache_files = list(self.cache_dir.glob("dashboard_metrics_*.json"))
            cache_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            for cache_file in cache_files:
                file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_time >= cutoff_time:
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            cached_metrics.append(data)
                    except Exception as e:
                        print(f"âš ï¸  ìºì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {cache_file}: {e}")
            
            return sorted(cached_metrics, key=lambda x: x.get('timestamp', ''))
            
        except Exception as e:
            print(f"âŒ ìºì‹œëœ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_health_status(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""
        return {
            'is_running': self._is_running,
            'subscribers_count': len(self._subscribers),
            'last_update': self._latest_metrics.timestamp if self._latest_metrics else None,
            'config': {
                'update_interval': self.config.update_interval_seconds,
                'cache_duration': self.config.cache_duration_minutes,
                'real_time_updates': self.config.enable_real_time_updates
            },
            'cache_directory': str(self.cache_dir),
            'cache_files_count': len(list(self.cache_dir.glob("dashboard_metrics_*.json")))
        }
    
    def export_current_snapshot(self, filename: Optional[str] = None) -> str:
        """í˜„ì¬ ìƒíƒœì˜ ìŠ¤ëƒ…ìƒ· export"""
        if not self._latest_metrics:
            self.force_update()
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"dashboard_snapshot_{timestamp}.json"
        
        filepath = self.cache_dir / filename
        
        snapshot_data = {
            'export_timestamp': datetime.now().isoformat(),
            'pipeline_status': self.get_health_status(),
            'metrics': self._latest_metrics.to_dict() if self._latest_metrics else None,
            'additional_analytics': {
                'agent_ranking': self.metrics_calculator.get_agent_ranking(period_days=7),
                'cost_efficiency': self.metrics_calculator.get_cost_efficiency_analysis()
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“¸ ìŠ¤ëƒ…ìƒ· ì €ì¥: {filepath}")
        return str(filepath)


# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_global_pipeline: Optional[DashboardDataPipeline] = None

def get_dashboard_pipeline() -> DashboardDataPipeline:
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_pipeline
    if _global_pipeline is None:
        _global_pipeline = DashboardDataPipeline()
    return _global_pipeline

def start_dashboard_pipeline(config: Optional[PipelineConfig] = None):
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
    global _global_pipeline
    if config:
        _global_pipeline = DashboardDataPipeline(config)
    else:
        _global_pipeline = get_dashboard_pipeline()
    
    _global_pipeline.start()
    return _global_pipeline

def stop_dashboard_pipeline():
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
    global _global_pipeline
    if _global_pipeline:
        _global_pipeline.stop()