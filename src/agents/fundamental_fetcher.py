"""
í€ë”ë©˜í„¸ í˜ì²˜ ì—ì´ì „íŠ¸ (Gemini 2.5 Flash-Lite)

ì¬ë¬´ì œí‘œ, ê³µì‹œ ì •ë³´, ë‰´ìŠ¤ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  
Gemini 2.5 Flash-Liteë¥¼ í™œìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸
"""

import logging
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import asdict

from src.agents.base_agent import BaseAgent, AgentContext
from src.utils.fundamental_data_engine import (
    FundamentalDataEngine, FundamentalData, FinancialRatio, 
    NewsData, IndustryComparison, DataQuality
)


class FundamentalFetcherAgent(BaseAgent):
    """í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        í€ë”ë©˜í„¸ í˜ì²˜ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì—ì´ì „íŠ¸ ì„¤ì •
        """
        # BaseAgent ì´ˆê¸°í™” (agent_llm_mapping.yamlì˜ fundamental_fetcher ì„¤ì • ì‚¬ìš©)
        super().__init__("fundamental_fetcher", config)
        
        # í€ë”ë©˜í„¸ ë°ì´í„° ì—”ì§„ ì´ˆê¸°í™”
        engine_config = config or {}
        self.data_engine = FundamentalDataEngine(
            cache_ttl_minutes=engine_config.get('cache_ttl_minutes', 5),
            mock_mode=engine_config.get('mock_mode', False)
        )
        
        # ì—ì´ì „íŠ¸ ì„¤ì •
        self.max_tickers_per_request = engine_config.get('max_tickers_per_request', 10)
        self.include_ai_analysis = engine_config.get('include_ai_analysis', True)
        self.analysis_depth = engine_config.get('analysis_depth', 'medium')  # light, medium, deep
        
        self.logger.info("âœ… í€ë”ë©˜í„¸ í˜ì²˜ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _validate_input(self, input_data: Dict[str, Any]) -> None:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        super()._validate_input(input_data)
        
        if 'tickers' not in input_data:
            raise ValueError("'tickers' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        tickers = input_data['tickers']
        if not isinstance(tickers, list) or len(tickers) == 0:
            raise ValueError("'tickers'ëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        if len(tickers) > self.max_tickers_per_request:
            raise ValueError(f"í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì¢…ëª© ìˆ˜ëŠ” ìµœëŒ€ {self.max_tickers_per_request}ê°œì…ë‹ˆë‹¤")
        
        # ì¢…ëª© ì½”ë“œ í˜•ì‹ ê²€ì¦
        for ticker in tickers:
            if not isinstance(ticker, str) or len(ticker) != 6 or not ticker.isdigit():
                raise ValueError(f"ì˜ëª»ëœ ì¢…ëª© ì½”ë“œ í˜•ì‹: {ticker}")
    
    def _validate_output(self, output_data: Dict[str, Any]) -> None:
        """ì¶œë ¥ ë°ì´í„° ê²€ì¦"""
        super()._validate_output(output_data)
        
        required_fields = ['fundamental_data', 'analysis_summary', 'data_quality', 'collection_stats']
        for field in required_fields:
            if field not in output_data:
                raise ValueError(f"ì¶œë ¥ ë°ì´í„°ì— '{field}' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _process(self, context: AgentContext) -> Dict[str, Any]:
        """
        í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì²˜ë¦¬
        
        Args:
            context: ì—ì´ì „íŠ¸ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„°
        """
        input_data = context.input_data
        tickers = input_data['tickers']
        
        # ì„ íƒì  ì„¤ì •
        include_news = input_data.get('include_news', True)
        include_industry_comparison = input_data.get('include_industry_comparison', True)
        sector_context = input_data.get('sector_context', {})  # ì„¹í„° ë¦¬ì„œì¹˜ ê²°ê³¼
        screening_context = input_data.get('screening_context', {})  # í‹°ì»¤ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼
        
        self.logger.info(f"ğŸ” í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {len(tickers)}ê°œ ì¢…ëª©")
        
        # 1. í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘
        fundamental_data_dict = self._collect_fundamental_data(
            tickers, include_news, include_industry_comparison
        )
        
        # 2. AI ê¸°ë°˜ ë¶„ì„ ë° ìš”ì•½ (Gemini 2.5 Flash-Lite)
        analysis_summary = {}
        if self.include_ai_analysis:
            analysis_summary = self._perform_ai_analysis(
                fundamental_data_dict, sector_context, screening_context
            )
        
        # 3. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        data_quality = self._assess_data_quality(fundamental_data_dict)
        
        # 4. ìˆ˜ì§‘ í†µê³„
        collection_stats = self._calculate_collection_stats(fundamental_data_dict)
        
        # 5. ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'fundamental_data': {ticker: asdict(data) for ticker, data in fundamental_data_dict.items()},
            'analysis_summary': analysis_summary,
            'data_quality': data_quality,
            'collection_stats': collection_stats,
            'metadata': {
                'processed_tickers': tickers,
                'total_tickers': len(tickers),
                'processing_timestamp': datetime.now().isoformat(),
                'agent_version': "1.0.0",
                'engine_config': {
                    'cache_ttl_minutes': self.data_engine.cache_ttl_minutes,
                    'mock_mode': self.data_engine.mock_mode,
                    'kis_authenticated': self.data_engine.kis_authenticated
                }
            }
        }
        
        self.logger.info(f"âœ… í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(fundamental_data_dict)}ê°œ ì¢…ëª©")
        return result_data
    
    def _collect_fundamental_data(
        self, 
        tickers: List[str], 
        include_news: bool, 
        include_industry_comparison: bool
    ) -> Dict[str, FundamentalData]:
        """í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘"""
        fundamental_data = {}
        
        for ticker in tickers:
            try:
                self.logger.info(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì¤‘: {ticker}")
                
                data = self.data_engine.collect_fundamental_data(
                    ticker=ticker,
                    include_news=include_news,
                    include_industry_comparison=include_industry_comparison
                )
                
                fundamental_data[ticker] = data
                
            except Exception as e:
                self.logger.error(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {e}")
                # ì‹¤íŒ¨í•œ ì¢…ëª©ë„ Mock ë°ì´í„°ë¡œ í¬í•¨
                try:
                    mock_data = self.data_engine._generate_mock_fundamental_data(ticker)
                    fundamental_data[ticker] = mock_data
                except Exception as mock_error:
                    self.logger.error(f"âŒ Mock ë°ì´í„° ìƒì„±ë„ ì‹¤íŒ¨: {ticker} - {mock_error}")
        
        return fundamental_data
    
    def _perform_ai_analysis(
        self, 
        fundamental_data: Dict[str, FundamentalData],
        sector_context: Dict[str, Any],
        screening_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Gemini 2.5 Flash-Liteë¥¼ í™œìš©í•œ AI ë¶„ì„"""
        try:
            # ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            analysis_prompt = self._build_analysis_prompt(
                fundamental_data, sector_context, screening_context
            )
            
            # Gemini 2.5 Flash-Liteë¡œ ë¶„ì„ ìš”ì²­
            response = self.query_llm(
                prompt=analysis_prompt,
                temperature=0.4,  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•œ ë‚®ì€ temperature
                max_tokens=2000
            )
            
            # ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
            analysis_result = self._parse_ai_analysis_response(response.content)
            
            return {
                'ai_analysis': analysis_result,
                'llm_metadata': {
                    'model_used': response.model_used,
                    'tokens_used': response.tokens_used,
                    'cost': response.cost,
                    'response_time': response.response_time,
                    'confidence_score': response.confidence_score
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'ai_analysis': {
                    'error': str(e),
                    'fallback_summary': self._generate_fallback_analysis(fundamental_data)
                },
                'llm_metadata': {'error': str(e)}
            }
    
    def _build_analysis_prompt(
        self, 
        fundamental_data: Dict[str, FundamentalData],
        sector_context: Dict[str, Any],
        screening_context: Dict[str, Any]
    ) -> str:
        """AI ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ê¸°ë³¸ ë°ì´í„° ìš”ì•½
        tickers = list(fundamental_data.keys())
        data_summary = []
        
        for ticker in tickers[:5]:  # ìµœëŒ€ 5ê°œ ì¢…ëª©ë§Œ ìƒì„¸ ë¶„ì„
            data = fundamental_data[ticker]
            fr = data.financial_ratios
            
            ticker_summary = f"""
ì¢…ëª© {ticker} ({data.company_name}):
- ì¬ë¬´ì§€í‘œ: PER={fr.per}, PBR={fr.pbr}, ROE={fr.roe}%, ë¶€ì±„ë¹„ìœ¨={fr.debt_ratio}%
- ìˆ˜ìµì„±: ì˜ì—…ì´ìµë¥ ={fr.operating_margin}%, ìˆœì´ìµë¥ ={fr.net_margin}%
- ì„±ì¥ì„±: ë§¤ì¶œì„±ì¥ë¥ ={fr.revenue_growth}%, ìˆœì´ìµì„±ì¥ë¥ ={fr.profit_growth}%
- ìµœê·¼ ë‰´ìŠ¤: {len(data.news_data)}ê±´ (í‰ê·  ê°ì •ì ìˆ˜: {sum(n.sentiment_score for n in data.news_data)/len(data.news_data) if data.news_data else 0:.2f})
- ì—…ê³„ ìˆœìœ„: {data.industry_comparison.rank_in_industry}/{data.industry_comparison.total_companies} (ìƒìœ„ {data.industry_comparison.percentile:.1f}%)
"""
            data_summary.append(ticker_summary)
        
        # ì„¹í„° ì»¨í…ìŠ¤íŠ¸ ìš”ì•½
        sector_summary = ""
        if sector_context:
            sector_summary = f"""
ì„¹í„° ë¦¬ì„œì¹˜ ê²°ê³¼:
- ìœ ë§ ì„¹í„°: {sector_context.get('top_sectors', [])}
- ì‹œì¥ íŠ¸ë Œë“œ: {sector_context.get('market_trends', 'N/A')}
- ì„¹í„° ëª¨ë©˜í…€: {sector_context.get('sector_momentum', 'N/A')}
"""
        
        # ìŠ¤í¬ë¦¬ë‹ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½
        screening_summary = ""
        if screening_context:
            screening_summary = f"""
í‹°ì»¤ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼:
- ì„ ë³„ ê¸°ì¤€: {screening_context.get('selection_criteria', 'N/A')}
- ìˆœìœ„ ë°©ë²•: {screening_context.get('ranking_method', 'N/A')}
- í•„í„°ë§ ì¡°ê±´: {screening_context.get('filtering_conditions', 'N/A')}
"""
        
        prompt = f"""ë‹¹ì‹ ì€ í€ë”ë©˜í„¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¢…ëª©ë“¤ì˜ í€ë”ë©˜í„¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ê´€ì ì—ì„œ ì¢…í•©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

{sector_summary}
{screening_summary}

== ë¶„ì„ ëŒ€ìƒ ì¢…ëª© ë°ì´í„° ==
{''.join(data_summary)}

== ë¶„ì„ ìš”ì²­ ì‚¬í•­ ==
1. **ì¢…ëª©ë³„ í€ë”ë©˜í„¸ í‰ê°€**: ê° ì¢…ëª©ì˜ ì¬ë¬´ ê±´ì „ì„±ê³¼ íˆ¬ì ë§¤ë ¥ë„ í‰ê°€
2. **ìƒëŒ€ì  ë¹„êµ ë¶„ì„**: ì¢…ëª© ê°„ ë¹„êµë¥¼ í†µí•œ ìš°ì„ ìˆœìœ„ ì œì‹œ
3. **ë¦¬ìŠ¤í¬ ìš”ì¸ ì‹ë³„**: ì£¼ìš” íˆ¬ì ë¦¬ìŠ¤í¬ì™€ ì£¼ì˜ì‚¬í•­
4. **íˆ¬ì ì „ëµ ì œì•ˆ**: í€ë”ë©˜í„¸ ê´€ì ì—ì„œì˜ íˆ¬ì ì „ëµ ë° íƒ€ì´ë°

== ì‘ë‹µ í˜•ì‹ ==
JSON í˜•íƒœë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "overall_assessment": "ì „ì²´ì ì¸ í€ë”ë©˜í„¸ í‰ê°€",
  "top_picks": ["ìƒìœ„ ì¶”ì²œ ì¢…ëª© 3ê°œ"],
  "risk_warnings": ["ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ë“¤"],
  "investment_strategy": "í€ë”ë©˜í„¸ ê¸°ë°˜ íˆ¬ì ì „ëµ",
  "market_outlook": "ì‹œì¥ ì „ë§ ë° ê´€ë ¨ ì´ìŠˆ",
  "confidence_level": "ë¶„ì„ ì‹ ë¢°ë„ (1-10ì )"
}}

ë¶„ì„ì€ í•œêµ­ ì£¼ì‹ì‹œì¥ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        
        return prompt
    
    def _parse_ai_analysis_response(self, response_content: str) -> Dict[str, Any]:
        """AI ë¶„ì„ ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            if '{' in response_content and '}' in response_content:
                start = response_content.find('{')
                end = response_content.rfind('}') + 1
                json_str = response_content[start:end]
                
                analysis = json.loads(json_str)
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
                required_fields = {
                    'overall_assessment': 'ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'top_picks': [],
                    'risk_warnings': [],
                    'investment_strategy': 'ì „ëµì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'market_outlook': 'ì „ë§ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'confidence_level': 5
                }
                
                for field, default in required_fields.items():
                    if field not in analysis:
                        analysis[field] = default
                
                return analysis
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ë¶„ì„
            return self._parse_text_analysis(response_content)
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                'overall_assessment': response_content[:500] + "..." if len(response_content) > 500 else response_content,
                'top_picks': [],
                'risk_warnings': ['AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜'],
                'investment_strategy': 'ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'market_outlook': 'íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•´ ì „ë§ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'confidence_level': 3,
                'parsing_error': str(e)
            }
    
    def _parse_text_analysis(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ í˜•íƒœ ì‘ë‹µì„ êµ¬ì¡°í™”"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì‹±
        lines = text.split('\n')
        
        assessment = ""
        strategy = ""
        
        for line in lines:
            if any(keyword in line.lower() for keyword in ['í‰ê°€', 'ì „ì²´', 'ì¢…í•©']):
                assessment += line + " "
            elif any(keyword in line.lower() for keyword in ['ì „ëµ', 'íˆ¬ì', 'ì¶”ì²œ']):
                strategy += line + " "
        
        return {
            'overall_assessment': assessment.strip() or text[:200] + "...",
            'top_picks': [],
            'risk_warnings': ['í…ìŠ¤íŠ¸ í˜•íƒœ ì‘ë‹µìœ¼ë¡œ ì¼ë¶€ ì •ë³´ ëˆ„ë½ ê°€ëŠ¥'],
            'investment_strategy': strategy.strip() or 'êµ¬ì²´ì ì¸ ì „ëµ ì •ë³´ ì—†ìŒ',
            'market_outlook': 'í…ìŠ¤íŠ¸ íŒŒì‹± ê²°ê³¼ ì „ë§ ì •ë³´ ì—†ìŒ',
            'confidence_level': 4
        }
    
    def _generate_fallback_analysis(self, fundamental_data: Dict[str, FundamentalData]) -> Dict[str, Any]:
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¶„ì„ ìƒì„±"""
        tickers = list(fundamental_data.keys())
        
        # ê°„ë‹¨í•œ ì ìˆ˜ ê¸°ë°˜ ìˆœìœ„
        scores = {}
        for ticker, data in fundamental_data.items():
            fr = data.financial_ratios
            score = 0
            
            # PER ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            if fr.per and fr.per > 0:
                score += max(0, 25 - fr.per) / 25 * 20
            
            # ROE ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            if fr.roe and fr.roe > 0:
                score += min(fr.roe, 20) / 20 * 20
            
            # ë¶€ì±„ë¹„ìœ¨ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            if fr.debt_ratio and fr.debt_ratio > 0:
                score += max(0, 50 - fr.debt_ratio) / 50 * 15
            
            # ë‰´ìŠ¤ ê°ì • ì ìˆ˜
            if data.news_data:
                avg_sentiment = sum(n.sentiment_score for n in data.news_data) / len(data.news_data)
                score += (avg_sentiment + 1) / 2 * 10  # -1~1ì„ 0~10ìœ¼ë¡œ ë³€í™˜
            
            scores[ticker] = score
        
        # ìƒìœ„ 3ê°œ ì¢…ëª©
        top_picks = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:3]
        
        return {
            'overall_assessment': f'{len(fundamental_data)}ê°œ ì¢…ëª© ì¤‘ ê¸°ë³¸ ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.',
            'top_picks': top_picks,
            'risk_warnings': ['AI ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ë§Œ ì œê³µë©ë‹ˆë‹¤.'],
            'investment_strategy': 'ì¬ë¬´ì§€í‘œ ê¸°ë°˜ ìƒìœ„ ì¢…ëª©ì„ ìš°ì„  ê²€í† í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.',
            'market_outlook': 'AI ë¶„ì„ ì—†ì´ëŠ” ì‹œì¥ ì „ë§ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
            'confidence_level': 2
        }
    
    def _assess_data_quality(self, fundamental_data: Dict[str, FundamentalData]) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        if not fundamental_data:
            return {'overall_quality': 'poor', 'quality_score': 0.0}
        
        quality_scores = []
        source_distribution = {}
        
        for ticker, data in fundamental_data.items():
            # ê°œë³„ ì¢…ëª© í’ˆì§ˆ ì ìˆ˜
            ticker_score = data.confidence_score
            quality_scores.append(ticker_score)
            
            # ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬
            source = data.financial_ratios.source
            source_distribution[source] = source_distribution.get(source, 0) + 1
        
        avg_quality = sum(quality_scores) / len(quality_scores)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if avg_quality >= 0.8:
            quality_grade = 'excellent'
        elif avg_quality >= 0.6:
            quality_grade = 'good'
        elif avg_quality >= 0.4:
            quality_grade = 'fair'
        else:
            quality_grade = 'poor'
        
        return {
            'overall_quality': quality_grade,
            'quality_score': round(avg_quality, 3),
            'individual_scores': {ticker: round(score, 3) for ticker, score in zip(fundamental_data.keys(), quality_scores)},
            'source_distribution': source_distribution,
            'total_tickers': len(fundamental_data),
            'assessment_timestamp': datetime.now().isoformat()
        }
    
    def _calculate_collection_stats(self, fundamental_data: Dict[str, FundamentalData]) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„ ê³„ì‚°"""
        if not fundamental_data:
            return {}
        
        # ìˆ˜ì§‘ ì‹œê°„ í†µê³„
        collection_times = [data.collection_time for data in fundamental_data.values()]
        avg_collection_time = sum(collection_times) / len(collection_times)
        
        # ë°ì´í„° ì™„ì„±ë„ í†µê³„
        completeness_scores = []
        for data in fundamental_data.values():
            fr = data.financial_ratios
            fields = [fr.per, fr.pbr, fr.roe, fr.debt_ratio, fr.operating_margin]
            completed_fields = sum(1 for field in fields if field is not None)
            completeness = completed_fields / len(fields)
            completeness_scores.append(completeness)
        
        avg_completeness = sum(completeness_scores) / len(completeness_scores)
        
        # ë‰´ìŠ¤ í†µê³„
        news_stats = {
            'total_news': sum(len(data.news_data) for data in fundamental_data.values()),
            'avg_news_per_ticker': sum(len(data.news_data) for data in fundamental_data.values()) / len(fundamental_data),
            'avg_sentiment': 0.0
        }
        
        all_sentiments = []
        for data in fundamental_data.values():
            for news in data.news_data:
                all_sentiments.append(news.sentiment_score)
        
        if all_sentiments:
            news_stats['avg_sentiment'] = sum(all_sentiments) / len(all_sentiments)
        
        # ìºì‹œ í†µê³„
        cache_stats = self.data_engine.get_cache_stats()
        
        return {
            'collection_performance': {
                'avg_collection_time': round(avg_collection_time, 3),
                'min_collection_time': round(min(collection_times), 3),
                'max_collection_time': round(max(collection_times), 3),
                'total_collection_time': round(sum(collection_times), 3)
            },
            'data_completeness': {
                'avg_completeness': round(avg_completeness, 3),
                'completeness_scores': {ticker: round(score, 3) for ticker, score in zip(fundamental_data.keys(), completeness_scores)}
            },
            'news_statistics': news_stats,
            'cache_statistics': cache_stats,
            'timestamp': datetime.now().isoformat()
        }
    
    def _calculate_confidence(self, result_data: Dict[str, Any]) -> float:
        """ê²°ê³¼ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not result_data.get('fundamental_data'):
            return 0.0
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì‹ ë¢°ë„
        data_quality_score = result_data.get('data_quality', {}).get('quality_score', 0.0)
        
        # AI ë¶„ì„ ì‹ ë¢°ë„
        ai_confidence = 0.0
        if 'analysis_summary' in result_data and 'ai_analysis' in result_data['analysis_summary']:
            ai_analysis = result_data['analysis_summary']['ai_analysis']
            if isinstance(ai_analysis, dict) and 'confidence_level' in ai_analysis:
                ai_confidence = ai_analysis['confidence_level'] / 10.0
        
        # ì „ì²´ ì‹ ë¢°ë„ (ë°ì´í„° í’ˆì§ˆ 70% + AI ë¶„ì„ 30%)
        overall_confidence = data_quality_score * 0.7 + ai_confidence * 0.3
        
        return round(overall_confidence, 3)


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_fundamental_fetcher(mock_mode: bool = False) -> FundamentalFetcherAgent:
    """í€ë”ë©˜í„¸ í˜ì²˜ ì—ì´ì „íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = {'mock_mode': mock_mode}
    return FundamentalFetcherAgent(config)


def fetch_fundamental_data(tickers: List[str], mock_mode: bool = False) -> Dict[str, Any]:
    """í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ í¸ì˜ í•¨ìˆ˜"""
    from src.agents.base_agent import AgentContext
    from datetime import datetime
    
    agent = create_fundamental_fetcher(mock_mode)
    
    context = AgentContext(
        agent_id="fundamental_fetcher",
        execution_id=f"fetch_{int(time.time())}",
        timestamp=datetime.now(),
        input_data={'tickers': tickers}
    )
    
    result = agent.execute(context)
    return result.result_data if result.success else {}


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    # ë‹¨ì¼ ì¢…ëª© í…ŒìŠ¤íŠ¸
    print("ğŸ§ª í€ë”ë©˜í„¸ í˜ì²˜ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    tickers = ["005930", "000660", "035420"]
    
    result = fetch_fundamental_data(tickers, mock_mode=True)
    
    if result:
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(result.get('fundamental_data', {}))}ê°œ ì¢…ëª© ì²˜ë¦¬")
        print(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ: {result.get('data_quality', {}).get('overall_quality', 'unknown')}")
        
        if 'analysis_summary' in result:
            ai_analysis = result['analysis_summary'].get('ai_analysis', {})
            if isinstance(ai_analysis, dict):
                print(f"ğŸ¤– AI ë¶„ì„ ì‹ ë¢°ë„: {ai_analysis.get('confidence_level', 'N/A')}")
                print(f"ğŸ“ˆ ì¶”ì²œ ì¢…ëª©: {ai_analysis.get('top_picks', [])}")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")