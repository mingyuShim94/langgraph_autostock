"""
ì„¹í„° ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸

Phase 2.1 ë…¸ë“œ 2: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì„¹í„° íŠ¸ë Œë“œ ë¶„ì„ ë° ìœ ë§ ì„¹í„° ë°œêµ´
Perplexity sonar-pro ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ì‹œì¥ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •í™•í•œ ì„¹í„° ë¶„ì„ ì œê³µ
"""

import logging
import time
from typing import Dict, Any, List, Optional
from datetime import datetime

from .base_agent import BaseAgent, AgentContext, AgentResult
from ..utils.sector_research_engine import get_sector_research_engine, SectorResearchResult
from ..llm_clients.base import LLMRequest

logger = logging.getLogger(__name__)


class SectorResearcherAgent(BaseAgent):
    """ì„¹í„° ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ì„¹í„° ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì¶”ê°€ ì„¤ì • (LLM ì„¤ì • ë®ì–´ì“°ê¸° ë“±)
        """
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œë„ (API í‚¤ê°€ ì—†ëŠ” ê²½ìš° mock ëª¨ë“œë¡œ í´ë°±)
        try:
            super().__init__("sector_researcher", config)
            self.llm_available = True
            logger.info(f"Perplexity í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ: {self.llm_client.model_name}")
        except Exception as e:
            logger.warning(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨, mock ëª¨ë“œë¡œ ì§„í–‰: {str(e)}")
            self.agent_type = "sector_researcher"
            self.config = config or {}
            self.logger = logging.getLogger(f"agent.{self.agent_type}")
            self.llm_client = None
            self.llm_available = False
        
        # ì˜ì¡´ì„± ì´ˆê¸°í™”
        self.research_engine = get_sector_research_engine()
        
        # ì—ì´ì „íŠ¸ë³„ ì„¤ì •
        self.default_focus_sectors = [
            "ê¸°ìˆ ì£¼", "ë°˜ë„ì²´", "ê¸ˆìœµ", "ìë™ì°¨", "í™”í•™", 
            "í—¬ìŠ¤ì¼€ì–´", "ì—ë„ˆì§€", "ì†Œì¬", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "í†µì‹ "
        ]
        
        self.analysis_cache = {}  # ìºì‹± ì‹œìŠ¤í…œ
        self.cache_ttl_minutes = 5  # ìºì‹œ ìœ íš¨ ì‹œê°„
        
        logger.info("ì„¹í„° ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _validate_input(self, input_data: Dict[str, Any]) -> None:
        """
        ì…ë ¥ ë°ì´í„° ê²€ì¦
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„°
            
        Raises:
            ValueError: ì…ë ¥ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        """
        super()._validate_input(input_data)
        
        # ì„¹í„° ë¦¬ì„œì¹˜ íŠ¹í™” ê²€ì¦
        if 'focus_sectors' in input_data:
            focus_sectors = input_data['focus_sectors']
            if not isinstance(focus_sectors, list):
                raise ValueError("focus_sectorsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
            
            # ìœ íš¨í•œ ì„¹í„°ì¸ì§€ í™•ì¸
            valid_sectors = self.research_engine.sectors
            invalid_sectors = [s for s in focus_sectors if s not in valid_sectors]
            if invalid_sectors:
                raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì„¹í„°: {invalid_sectors}")
        
        # ë¶„ì„ ê¹Šì´ ì„¤ì • ê²€ì¦
        if 'analysis_depth' in input_data:
            depth = input_data['analysis_depth']
            if depth not in ['basic', 'detailed', 'comprehensive']:
                raise ValueError("analysis_depthëŠ” 'basic', 'detailed', 'comprehensive' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    def _process(self, context: AgentContext) -> Dict[str, Any]:
        """
        ì„¹í„° ë¦¬ì„œì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ì„¹í„° ë¦¬ì„œì¹˜ ê²°ê³¼
        """
        input_data = context.input_data
        
        # ì…ë ¥ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        focus_sectors = input_data.get('focus_sectors', self.default_focus_sectors)
        analysis_depth = input_data.get('analysis_depth', 'detailed')
        force_refresh = input_data.get('force_refresh', False)
        
        logger.info(f"ì„¹í„° ë¦¬ì„œì¹˜ ì‹œì‘ - ëŒ€ìƒ ì„¹í„°: {len(focus_sectors)}ê°œ, ê¹Šì´: {analysis_depth}")
        
        try:
            # ìºì‹œ í™•ì¸ (ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš°)
            cache_key = self._generate_cache_key(focus_sectors, analysis_depth)
            if not force_refresh and self._is_cache_valid(cache_key):
                logger.info("ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©")
                cached_result = self.analysis_cache[cache_key]
                return cached_result['data']
            
            # ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
            if self.llm_available:
                research_result = self._perform_real_time_analysis(focus_sectors, analysis_depth)
            else:
                logger.info("Mock ëª¨ë“œë¡œ ì„¹í„° ë¶„ì„ ìˆ˜í–‰")
                research_result = self._perform_mock_analysis(focus_sectors, analysis_depth)
            
            # ê²°ê³¼ ê²€ì¦
            is_valid, validation_errors = self.research_engine.validate_research_result(research_result)
            if not is_valid:
                logger.warning(f"ë¶„ì„ ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {validation_errors}")
                # ê²½ê³ ëŠ” í•˜ì§€ë§Œ ì§„í–‰ (ë¶€ë¶„ì  ê²°ê³¼ë¼ë„ ì œê³µ)
            
            # ê²°ê³¼ êµ¬ì¡°í™”
            result_data = {
                'research_result': research_result.to_dict(),
                'analysis_metadata': {
                    'agent_type': self.agent_type,
                    'analysis_depth': analysis_depth,
                    'sectors_analyzed': len(focus_sectors),
                    'llm_mode': 'real' if self.llm_available else 'mock',
                    'cache_used': False,
                    'validation_passed': is_valid,
                    'validation_errors': validation_errors if not is_valid else None
                },
                'summary': self._generate_summary(research_result),
                'recommendations': self._generate_recommendations(research_result)
            }
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self._update_cache(cache_key, result_data)
            
            logger.info(f"ì„¹í„° ë¦¬ì„œì¹˜ ì™„ë£Œ - ë¶„ì„ëœ ì„¹í„°: {len(research_result.sector_rankings)}ê°œ")
            return result_data
            
        except Exception as e:
            logger.error(f"ì„¹í„° ë¦¬ì„œì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def _perform_real_time_analysis(self, focus_sectors: List[str], analysis_depth: str) -> SectorResearchResult:
        """
        ì‹¤ì‹œê°„ Perplexity ê¸°ë°˜ ì„¹í„° ë¶„ì„
        
        Args:
            focus_sectors: ë¶„ì„ ëŒ€ìƒ ì„¹í„°
            analysis_depth: ë¶„ì„ ê¹Šì´
            
        Returns:
            SectorResearchResult: ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"Perplexityë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì„¹í„° ë¶„ì„ ì‹œì‘ - {self.llm_client.model_name}")
        
        # ë¶„ì„ ê¹Šì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if analysis_depth == 'basic':
            sectors_to_analyze = focus_sectors[:5]  # ìƒìœ„ 5ê°œë§Œ
        elif analysis_depth == 'detailed':
            sectors_to_analyze = focus_sectors[:10]  # ìƒìœ„ 10ê°œ
        else:  # comprehensive
            sectors_to_analyze = focus_sectors  # ì „ì²´
        
        # ë¶„ì„ ê¹Šì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ íƒ€ì… ê²°ì •
        analysis_type_mapping = {
            'basic': 'market_overview',
            'detailed': 'comprehensive', 
            'comprehensive': 'comprehensive'
        }
        prompt_type = analysis_type_mapping.get(analysis_depth, 'comprehensive')
        
        # Perplexity ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.research_engine.generate_sector_research_prompt(
            focus_sectors=sectors_to_analyze,
            analysis_type=prompt_type
        )
        
        # LLM ìš”ì²­ ìƒì„±
        llm_request = LLMRequest(
            prompt=prompt,
            agent_type=self.agent_type,
            max_tokens=4000 if analysis_depth == 'comprehensive' else 3000,
            temperature=0.3,  # íŒ©íŠ¸ ì²´í¬ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ì€ temperature
            metadata={
                'analysis_depth': analysis_depth,
                'sectors_count': len(sectors_to_analyze),
                'request_type': 'sector_research'
            }
        )
        
        # Perplexity í˜¸ì¶œ
        start_time = time.time()
        llm_response = self.llm_client.generate_response(llm_request)
        response_time = time.time() - start_time
        
        logger.info(f"Perplexity ì‘ë‹µ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ, "
                   f"í† í°: {llm_response.tokens_used}, ë¹„ìš©: ${llm_response.cost:.4f}")
        
        # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
        if llm_response.confidence_score < 0.7:
            logger.warning(f"ë‚®ì€ ì‘ë‹µ í’ˆì§ˆ - ì‹ ë¢°ë„: {llm_response.confidence_score:.2f}")
        
        # ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
        research_result = self.research_engine.parse_llm_response(llm_response.content)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        research_result.confidence_indicators.update({
            'llm_confidence': llm_response.confidence_score,
            'response_time': response_time,
            'tokens_used': llm_response.tokens_used,
            'has_citations': llm_response.metadata.get('has_citations', False),
            'search_enhanced': llm_response.metadata.get('search_enhanced', True)
        })
        
        return research_result
    
    def _perform_mock_analysis(self, focus_sectors: List[str], analysis_depth: str) -> SectorResearchResult:
        """
        Mock ëª¨ë“œ ì„¹í„° ë¶„ì„ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            focus_sectors: ë¶„ì„ ëŒ€ìƒ ì„¹í„°
            analysis_depth: ë¶„ì„ ê¹Šì´
            
        Returns:
            SectorResearchResult: Mock ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"Mock ëª¨ë“œë¡œ ì„¹í„° ë¶„ì„ ìˆ˜í–‰ - ëŒ€ìƒ: {len(focus_sectors)}ê°œ ì„¹í„°")
        
        # Mock ì‘ë‹µ ìƒì„± (ì‹¤ì œ LLM ì‘ë‹µì„ ì‹œë®¬ë ˆì´ì…˜)
        mock_response = f"""
        Mock ì„¹í„° ë¶„ì„ ê²°ê³¼ ({datetime.now().strftime('%Y-%m-%d %H:%M')})
        
        ë¶„ì„ ëŒ€ìƒ ì„¹í„°: {', '.join(focus_sectors[:5])}
        
        ì£¼ìš” ë°œê²¬ì‚¬í•­:
        1. {focus_sectors[0]} ì„¹í„°ê°€ í˜„ì¬ ê°€ì¥ ë†’ì€ ì„±ì¥ ì ì¬ë ¥ì„ ë³´ì„
        2. ì •ë¶€ ì •ì±… ë³€í™”ë¡œ {focus_sectors[1]} ì„¹í„°ì— ê¸ì •ì  ì˜í–¥ ì˜ˆìƒ
        3. ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©ìœ¼ë¡œ ì¸í•´ {focus_sectors[2]} ì„¹í„°ëŠ” ì£¼ì˜ í•„ìš”
        
        íˆ¬ì ì¶”ì²œ: {focus_sectors[0]}, {focus_sectors[1]} ì„¹í„° ìš°ì„  ê³ ë ¤
        """
        
        # Mock ê²°ê³¼ë¥¼ ì‹¤ì œ ê²°ê³¼ í˜•íƒœë¡œ íŒŒì‹±
        research_result = self.research_engine.parse_llm_response(mock_response)
        
        # Mock íŠ¹í™” ë©”íƒ€ë°ì´í„°
        research_result.confidence_indicators.update({
            'mock_mode': True,
            'data_freshness': 0.5,  # Mockì€ ì‹ ì„ ë„ê°€ ë‚®ìŒ
            'source_reliability': 0.6  # Mockì€ ì‹ ë¢°ë„ê°€ ë‚®ìŒ
        })
        
        return research_result
    
    def _generate_summary(self, research_result: SectorResearchResult) -> str:
        """
        ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
        
        Args:
            research_result: ì„¹í„° ë¦¬ì„œì¹˜ ê²°ê³¼
            
        Returns:
            str: ë¶„ì„ ìš”ì•½
        """
        top_sectors = research_result.sector_rankings[:3]
        
        summary = f"""
ì„¹í„° ë¦¬ì„œì¹˜ ìš”ì•½ ({research_result.research_timestamp.strftime('%Y-%m-%d %H:%M')})

ğŸ† ìµœê³  ì„¹í„° TOP 3:
1. {top_sectors[0].sector_name} (ì ìˆ˜: {top_sectors[0].trend_score:.1f}/100)
2. {top_sectors[1].sector_name} (ì ìˆ˜: {top_sectors[1].trend_score:.1f}/100)  
3. {top_sectors[2].sector_name} (ì ìˆ˜: {top_sectors[2].trend_score:.1f}/100)

ğŸ’¡ ì£¼ìš” íˆ¬ì ê¸°íšŒ: {len(research_result.top_opportunities)}ê°œ ë°œêµ´
ğŸ”„ ì„¹í„° ë¡œí…Œì´ì…˜ ì‹ í˜¸: {len(research_result.rotation_signals)}ê°œ ê°ì§€

ğŸ“ˆ ì‹œì¥ ê°œìš”: {research_result.market_overview}

ì‹ ë¢°ë„: {research_result.confidence_indicators.get('analysis_confidence', 0.8):.1%}
        """.strip()
        
        return summary
    
    def _generate_recommendations(self, research_result: SectorResearchResult) -> List[Dict[str, Any]]:
        """
        íˆ¬ì ì¶”ì²œ ìƒì„±
        
        Args:
            research_result: ì„¹í„° ë¦¬ì„œì¹˜ ê²°ê³¼
            
        Returns:
            List[Dict[str, Any]]: ì¶”ì²œ ëª©ë¡
        """
        recommendations = []
        
        # ìƒìœ„ 3ê°œ ì„¹í„° ì¶”ì²œ
        for i, sector_metrics in enumerate(research_result.sector_rankings[:3]):
            recommendation = {
                'rank': i + 1,
                'sector': sector_metrics.sector_name,
                'action': 'BUY' if sector_metrics.trend_score >= 70 else 'HOLD',
                'confidence': sector_metrics.trend_score / 100,
                'reasoning': f"íŠ¸ë Œë“œ ì ìˆ˜ {sector_metrics.trend_score:.1f}ì , "
                           f"ê¸°íšŒ ìˆ˜ì¤€ {sector_metrics.opportunity_level}",
                'risk_level': sector_metrics.risk_level,
                'time_horizon': 'ì¤‘ê¸°' if sector_metrics.trend_score >= 80 else 'ë‹¨ê¸°'
            }
            recommendations.append(recommendation)
        
        # ê¸°íšŒ ê¸°ë°˜ ì¶”ì²œ ì¶”ê°€
        for opportunity in research_result.top_opportunities:
            if opportunity.confidence_level >= 0.7:
                recommendations.append({
                    'rank': len(recommendations) + 1,
                    'sector': opportunity.sector_name,
                    'action': 'WATCH',
                    'confidence': opportunity.confidence_level,
                    'reasoning': opportunity.description,
                    'risk_level': 'ë³´í†µ',
                    'time_horizon': opportunity.time_horizon,
                    'opportunity_type': opportunity.opportunity_type
                })
        
        return recommendations
    
    def _generate_cache_key(self, focus_sectors: List[str], analysis_depth: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        sectors_key = '_'.join(sorted(focus_sectors))
        return f"{self.agent_type}_{analysis_depth}_{hash(sectors_key) % 10000}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸"""
        if cache_key not in self.analysis_cache:
            return False
        
        cache_entry = self.analysis_cache[cache_key]
        cache_time = cache_entry['timestamp']
        current_time = datetime.now()
        
        return (current_time - cache_time).total_seconds() < (self.cache_ttl_minutes * 60)
    
    def _update_cache(self, cache_key: str, data: Dict[str, Any]) -> None:
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        self.analysis_cache[cache_key] = {
            'timestamp': datetime.now(),
            'data': data
        }
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 10ê°œ í•­ëª©)
        if len(self.analysis_cache) > 10:
            oldest_key = min(self.analysis_cache.keys(), 
                           key=lambda k: self.analysis_cache[k]['timestamp'])
            del self.analysis_cache[oldest_key]
    
    def _validate_output(self, output_data: Dict[str, Any]) -> None:
        """
        ì¶œë ¥ ë°ì´í„° ê²€ì¦
        
        Args:
            output_data: ì¶œë ¥ ë°ì´í„°
            
        Raises:
            ValueError: ì¶œë ¥ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        """
        super()._validate_output(output_data)
        
        # ì„¹í„° ë¦¬ì„œì¹˜ íŠ¹í™” ê²€ì¦
        required_fields = ['research_result', 'analysis_metadata', 'summary', 'recommendations']
        for field in required_fields:
            if field not in output_data:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ì¶œë ¥ì— ì—†ìŠµë‹ˆë‹¤")
        
        # ì—°êµ¬ ê²°ê³¼ ê²€ì¦
        research_result = output_data['research_result']
        if not research_result.get('sector_rankings'):
            raise ValueError("ì„¹í„° ìˆœìœ„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¶”ì²œ ë°ì´í„° ê²€ì¦
        recommendations = output_data['recommendations']
        if not isinstance(recommendations, list):
            raise ValueError("ì¶”ì²œ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    def _calculate_confidence(self, result_data: Dict[str, Any]) -> float:
        """
        ê²°ê³¼ì— ëŒ€í•œ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            result_data: ê²°ê³¼ ë°ì´í„°
            
        Returns:
            float: ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            research_result = result_data['research_result']
            confidence_indicators = research_result.get('confidence_indicators', {})
            
            # ì—¬ëŸ¬ ì‹ ë¢°ë„ ì§€í‘œì˜ ê°€ì¤‘ í‰ê· 
            base_confidence = confidence_indicators.get('analysis_confidence', 0.8)
            data_freshness = confidence_indicators.get('data_freshness', 0.8)
            source_reliability = confidence_indicators.get('source_reliability', 0.8)
            llm_confidence = confidence_indicators.get('llm_confidence', 0.8)
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = [0.3, 0.2, 0.2, 0.3]  # base, freshness, reliability, llm
            confidences = [base_confidence, data_freshness, source_reliability, llm_confidence]
            
            weighted_confidence = sum(w * c for w, c in zip(weights, confidences))
            
            # LLM ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë³´ì •
            if not self.llm_available:
                weighted_confidence *= 0.7  # Mock ëª¨ë“œëŠ” ì‹ ë¢°ë„ ê°ì†Œ
            
            return max(0.0, min(1.0, weighted_confidence))
            
        except Exception as e:
            logger.warning(f"ì‹ ë¢°ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.6  # ê¸°ë³¸ê°’
    
    def get_agent_info(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_agent_info()
        
        # ì„¹í„° ë¦¬ì„œì¹˜ íŠ¹í™” ì •ë³´ ì¶”ê°€
        base_info.update({
            'specialization': 'ì‹¤ì‹œê°„ ì„¹í„° íŠ¸ë Œë“œ ë¶„ì„',
            'supported_sectors': len(self.research_engine.sectors),
            'analysis_depths': ['basic', 'detailed', 'comprehensive'],
            'cache_ttl_minutes': self.cache_ttl_minutes,
            'real_time_search': self.llm_available,
            'default_focus_sectors': self.default_focus_sectors
        })
        
        return base_info
    
    def clear_cache(self) -> None:
        """ë¶„ì„ ìºì‹œ í´ë¦¬ì–´"""
        self.analysis_cache.clear()
        logger.info("ì„¹í„° ë¶„ì„ ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def get_cache_status(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'cache_entries': len(self.analysis_cache),
            'cache_ttl_minutes': self.cache_ttl_minutes,
            'cached_keys': list(self.analysis_cache.keys())
        }