#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‹¤ì¤‘ LLM ë™ì‹œ í˜¸ì¶œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

ì‹¤ì œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ì—ì„œ ë°œìƒí•˜ëŠ” ë™ì‹œ LLM í˜¸ì¶œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import time
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# pytestëŠ” ì„ íƒì ìœ¼ë¡œ import (ì—†ì–´ë„ í´ë˜ìŠ¤ëŠ” ì‘ë™)
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False

from src.llm_clients.client_factory import LLMClientFactory
from src.llm_clients.base import LLMRequest, LLMResponse


class MultiLLMWorkflowTester:
    """ë‹¤ì¤‘ LLM ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client_factory = LLMClientFactory()
        self.test_results = {}
    
    def create_test_request(self, agent_type: str, test_prompt: str = None) -> LLMRequest:
        """í…ŒìŠ¤íŠ¸ìš© LLM ìš”ì²­ ìƒì„±"""
        if test_prompt is None:
            test_prompt = f"This is a test request for {agent_type} agent. Please respond with a brief analysis."
        
        return LLMRequest(
            prompt=test_prompt,
            agent_type=agent_type,
            temperature=0.1,
            max_tokens=500
        )
    
    def test_single_llm_call(self, agent_type: str) -> Dict[str, Any]:
        """ë‹¨ì¼ LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            client = self.client_factory.get_client(agent_type)
            request = self.create_test_request(agent_type)
            response = client.generate_response(request)
            
            end_time = time.time()
            response_time = end_time - start_time
            
            return {
                'agent_type': agent_type,
                'success': True,
                'response_time': response_time,
                'tokens_used': response.tokens_used,
                'cost': response.cost,
                'model_used': response.model_used,
                'error': None
            }
            
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            return {
                'agent_type': agent_type,
                'success': False,
                'response_time': response_time,
                'tokens_used': 0,
                'cost': 0.0,
                'model_used': None,
                'error': str(e)
            }
    
    def test_parallel_analysis_agents(self) -> Dict[str, Any]:
        """ë³‘ë ¬ ë¶„ì„ ì—ì´ì „íŠ¸ ë™ì‹œ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (Phase 2 5a-5d)"""
        parallel_agents = [
            'valuation_analyst',
            'flow_analyst', 
            'risk_analyst',
            'technical_analyst'
        ]
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            # 4ê°œ ì—ì´ì „íŠ¸ ë™ì‹œ ì‹¤í–‰
            future_to_agent = {
                executor.submit(self.test_single_llm_call, agent): agent 
                for agent in parallel_agents
            }
            
            for future in as_completed(future_to_agent):
                agent = future_to_agent[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    results.append({
                        'agent_type': agent,
                        'success': False,
                        'response_time': 0,
                        'tokens_used': 0,
                        'cost': 0.0,
                        'model_used': None,
                        'error': f"Execution error: {str(e)}"
                    })
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ì§‘ê³„
        successful_calls = [r for r in results if r['success']]
        failed_calls = [r for r in results if not r['success']]
        
        return {
            'test_type': 'parallel_analysis',
            'total_time': total_time,
            'total_agents': len(parallel_agents),
            'successful_calls': len(successful_calls),
            'failed_calls': len(failed_calls),
            'success_rate': len(successful_calls) / len(parallel_agents),
            'average_response_time': sum(r['response_time'] for r in successful_calls) / len(successful_calls) if successful_calls else 0,
            'total_tokens': sum(r['tokens_used'] for r in results),
            'total_cost': sum(r['cost'] for r in results),
            'individual_results': results
        }
    
    def test_sequential_workflow(self) -> Dict[str, Any]:
        """ìˆœì°¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        sequential_agents = [
            'portfolio_rebalancer',
            'sector_researcher', 
            'ticker_screener',
            'fundamental_fetcher'
        ]
        
        start_time = time.time()
        results = []
        
        # ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        for agent in sequential_agents:
            result = self.test_single_llm_call(agent)
            results.append(result)
            
            # ì‹¤íŒ¨í•œ ê²½ìš° ì¤‘ë‹¨
            if not result['success']:
                break
        
        end_time = time.time()
        total_time = end_time - start_time
        
        successful_calls = [r for r in results if r['success']]
        failed_calls = [r for r in results if not r['success']]
        
        return {
            'test_type': 'sequential_workflow',
            'total_time': total_time,
            'total_agents': len(sequential_agents),
            'completed_agents': len(results),
            'successful_calls': len(successful_calls),
            'failed_calls': len(failed_calls),
            'success_rate': len(successful_calls) / len(results) if results else 0,
            'individual_results': results
        }
    
    def test_cio_decision_making(self) -> Dict[str, Any]:
        """CIO ì—ì´ì „íŠ¸ ë‹¨ë… ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸"""
        cio_prompt = """
        Based on the following analysis reports, make a final investment decision:
        
        Valuation Analysis: AAPL appears undervalued with P/E of 25 vs industry average of 30
        Flow Analysis: Strong institutional buying detected in tech sector
        Risk Analysis: Portfolio concentration risk is acceptable at current levels
        Technical Analysis: AAPL showing bullish breakout pattern above $150 resistance
        
        Please provide your final investment recommendation.
        """
        
        request = self.create_test_request('cio', cio_prompt)
        result = self.test_single_llm_call('cio')
        
        return {
            'test_type': 'cio_decision',
            'agent_type': 'cio',
            'result': result
        }
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ë‹¤ì¤‘ LLM ì›Œí¬í”Œë¡œìš° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {}
        
        # 1. ë³‘ë ¬ ë¶„ì„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
        print("ğŸ“Š ë³‘ë ¬ ë¶„ì„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['parallel_analysis'] = self.test_parallel_analysis_agents()
        
        # 2. ìˆœì°¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        print("ğŸ”„ ìˆœì°¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['sequential_workflow'] = self.test_sequential_workflow()
        
        # 3. CIO ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸
        print("ğŸ‘‘ CIO ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['cio_decision'] = self.test_cio_decision_making()
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        total_calls = 0
        total_successful = 0
        total_time = 0
        total_cost = 0
        
        for test_name, result in test_results.items():
            if test_name in ['parallel_analysis', 'sequential_workflow']:
                total_calls += result.get('total_agents', 0)
                total_successful += result.get('successful_calls', 0)
                total_time += result.get('total_time', 0)
                total_cost += result.get('total_cost', 0)
            elif test_name == 'cio_decision':
                total_calls += 1
                total_successful += 1 if result['result']['success'] else 0
                total_time += result['result']['response_time']
                total_cost += result['result']['cost']
        
        test_results['summary'] = {
            'total_calls': total_calls,
            'total_successful': total_successful,
            'overall_success_rate': total_successful / total_calls if total_calls > 0 else 0,
            'total_time': total_time,
            'total_cost': total_cost,
            'test_passed': total_successful / total_calls >= 0.8 if total_calls > 0 else False  # 80% ì„±ê³µë¥  ê¸°ì¤€
        }
        
        return test_results


# pytest í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
class TestMultiLLMWorkflow:
    """pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    
    @pytest.fixture
    def tester(self):
        """í…ŒìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return MultiLLMWorkflowTester()
    
    def test_parallel_analysis_agents_success_rate(self, tester):
        """ë³‘ë ¬ ë¶„ì„ ì—ì´ì „íŠ¸ ì„±ê³µë¥  í…ŒìŠ¤íŠ¸"""
        result = tester.test_parallel_analysis_agents()
        
        # 80% ì´ìƒ ì„±ê³µë¥  ìš”êµ¬
        assert result['success_rate'] >= 0.8, f"Success rate {result['success_rate']} is below 80%"
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ 60ì´ˆ ì´í•˜ ìš”êµ¬
        assert result['average_response_time'] <= 60, f"Average response time {result['average_response_time']} exceeds 60 seconds"
        
        print(f"âœ… ë³‘ë ¬ ë¶„ì„ í…ŒìŠ¤íŠ¸ í†µê³¼: ì„±ê³µë¥  {result['success_rate']:.1%}, í‰ê·  ì‘ë‹µì‹œê°„ {result['average_response_time']:.1f}ì´ˆ")
    
    def test_sequential_workflow_completion(self, tester):
        """ìˆœì°¨ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ í…ŒìŠ¤íŠ¸"""
        result = tester.test_sequential_workflow()
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ ì„±ê³µì  ì™„ë£Œ ìš”êµ¬
        assert result['success_rate'] >= 0.8, f"Sequential workflow success rate {result['success_rate']} is below 80%"
        
        print(f"âœ… ìˆœì°¨ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼: ì„±ê³µë¥  {result['success_rate']:.1%}")
    
    def test_cio_decision_making(self, tester):
        """CIO ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸"""
        result = tester.test_cio_decision_making()
        
        assert result['result']['success'], f"CIO decision making failed: {result['result']['error']}"
        
        print(f"âœ… CIO ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ í†µê³¼: ì‘ë‹µì‹œê°„ {result['result']['response_time']:.1f}ì´ˆ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = MultiLLMWorkflowTester()
    results = tester.run_comprehensive_test()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ë‹¤ì¤‘ LLM ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    
    # ë³‘ë ¬ ë¶„ì„ ê²°ê³¼
    parallel_result = results['parallel_analysis']
    print(f"\nğŸ“Š ë³‘ë ¬ ë¶„ì„ ì—ì´ì „íŠ¸ (4ê°œ ë™ì‹œ í˜¸ì¶œ):")
    print(f"   ì„±ê³µë¥ : {parallel_result['success_rate']:.1%} ({parallel_result['successful_calls']}/{parallel_result['total_agents']})")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {parallel_result['total_time']:.1f}ì´ˆ")
    print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {parallel_result['average_response_time']:.1f}ì´ˆ")
    print(f"   ì´ ë¹„ìš©: ${parallel_result['total_cost']:.4f}")
    
    # ìˆœì°¨ ì›Œí¬í”Œë¡œìš° ê²°ê³¼
    sequential_result = results['sequential_workflow']
    print(f"\nğŸ”„ ìˆœì°¨ ì›Œí¬í”Œë¡œìš°:")
    print(f"   ì„±ê³µë¥ : {sequential_result['success_rate']:.1%} ({sequential_result['successful_calls']}/{sequential_result['completed_agents']})")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {sequential_result['total_time']:.1f}ì´ˆ")
    
    # CIO ê²°ê³¼
    cio_result = results['cio_decision']['result']
    print(f"\nğŸ‘‘ CIO ì˜ì‚¬ê²°ì •:")
    print(f"   ì„±ê³µ: {'âœ…' if cio_result['success'] else 'âŒ'}")
    print(f"   ì‘ë‹µì‹œê°„: {cio_result['response_time']:.1f}ì´ˆ")
    print(f"   ì‚¬ìš© ëª¨ë¸: {cio_result['model_used']}")
    
    # ì „ì²´ ìš”ì•½
    summary = results['summary']
    print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
    print(f"   ì´ í˜¸ì¶œ ìˆ˜: {summary['total_calls']}")
    print(f"   ì „ì²´ ì„±ê³µë¥ : {summary['overall_success_rate']:.1%}")
    print(f"   ì´ ì†Œìš”ì‹œê°„: {summary['total_time']:.1f}ì´ˆ")
    print(f"   ì´ ë¹„ìš©: ${summary['total_cost']:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ í†µê³¼: {'âœ… PASS' if summary['test_passed'] else 'âŒ FAIL'}")
    
    return results


if __name__ == "__main__":
    main()