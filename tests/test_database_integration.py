#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸

ìƒˆë¡œ ì¶”ê°€ëœ í…Œì´ë¸”ë“¤(AgentPerformance, LLMUsageLog, ModelEvolutionHistory)ê³¼ 
ê¸°ì¡´ ì‹œìŠ¤í…œ ê°„ì˜ ë°ì´í„° ë¬´ê²°ì„± ë° í†µí•© ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# pytestëŠ” ì„ íƒì ìœ¼ë¡œ import
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False

from src.database.schema import (
    DatabaseManager, 
    TradeRecord, 
    AgentPerformance, 
    LLMUsageLog, 
    ModelEvolutionHistory
)


class DatabaseIntegrationTester:
    """ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ê¸°ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.test_session_id = f"test_integration_{int(time.time())}"
        self.test_data_ids = {
            'trade_records': [],
            'agent_performances': [],
            'llm_usage_logs': [],
            'model_evolution_histories': []
        }
    
    def create_sample_trade_record(self, trade_id: str = None) -> TradeRecord:
        """ìƒ˜í”Œ ê±°ë˜ ê¸°ë¡ ìƒì„±"""
        if trade_id is None:
            trade_id = f"trade_{self.test_session_id}_{len(self.test_data_ids['trade_records'])}"
        
        return TradeRecord(
            trade_id=trade_id,
            symbol="AAPL",
            action="BUY",
            quantity=100,
            price=150.50,
            total_amount=15050.0,
            timestamp=datetime.now(),
            market_analysis="Strong technical indicators and positive earnings outlook",
            trading_plan="Buy 100 shares at current market price",
            risk_validation="Risk acceptable: position size 2% of portfolio",
            execution_result="Order executed successfully",
            agent_contributions={
                "valuation_analyst": 0.3,
                "technical_analyst": 0.4,
                "risk_analyst": 0.2,
                "cio": 0.1
            },
            pnl=0.0,
            status="EXECUTED"
        )
    
    def create_sample_agent_performance(self, agent_type: str, trade_id: str) -> AgentPerformance:
        """ìƒ˜í”Œ ì—ì´ì „íŠ¸ ì„±ê³¼ ê¸°ë¡ ìƒì„±"""
        return AgentPerformance(
            agent_type=agent_type,
            trade_id=trade_id,
            decision_quality_score=0.85,
            contribution_weight=0.3,
            execution_time=2.5,
            confidence_level=0.9,
            analysis_accuracy=0.8,
            timestamp=datetime.now()
        )
    
    def create_sample_llm_usage_log(self, agent_type: str) -> LLMUsageLog:
        """ìƒ˜í”Œ LLM ì‚¬ìš©ëŸ‰ ë¡œê·¸ ìƒì„±"""
        return LLMUsageLog(
            session_id=self.test_session_id,
            agent_type=agent_type,
            provider="openai",
            model_name="gpt-4",
            tokens_used=1500,
            cost=0.03,
            response_time=3.2,
            timestamp=datetime.now()
        )
    
    def create_sample_model_evolution(self, agent_type: str) -> ModelEvolutionHistory:
        """ìƒ˜í”Œ ëª¨ë¸ ì§„í™” íˆìŠ¤í† ë¦¬ ìƒì„±"""
        return ModelEvolutionHistory(
            agent_type=agent_type,
            old_model="gpt-4",
            new_model="gpt-5",
            change_reason="Performance improvement in technical analysis",
            performance_improvement=0.15,
            timestamp=datetime.now()
        )
    
    def test_basic_crud_operations(self) -> Dict[str, Any]:
        """ê¸°ë³¸ CRUD ì‘ì—… í…ŒìŠ¤íŠ¸"""
        test_results = {
            'trade_records': {'create': False, 'read': False, 'update': False, 'delete': False},
            'agent_performances': {'create': False, 'read': False, 'update': False, 'delete': False},
            'llm_usage_logs': {'create': False, 'read': False, 'update': False, 'delete': False},
            'model_evolution_histories': {'create': False, 'read': False, 'update': False, 'delete': False}
        }
        
        # TradeRecord CRUD í…ŒìŠ¤íŠ¸
        try:
            # Create
            trade_record = self.create_sample_trade_record()
            trade_id = self.db_manager.add_trade_record(trade_record)
            self.test_data_ids['trade_records'].append(trade_id)
            test_results['trade_records']['create'] = True
            
            # Read
            retrieved_trade = self.db_manager.get_trade_record(trade_id)
            test_results['trade_records']['read'] = (retrieved_trade is not None)
            
            # Update
            if retrieved_trade:
                retrieved_trade.pnl = 500.0
                updated = self.db_manager.update_trade_pnl(trade_id, 500.0)
                test_results['trade_records']['update'] = updated
            
            # DeleteëŠ” ì‹¤ì œë¡œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ (ë°ì´í„° ë³´ì¡´)
            test_results['trade_records']['delete'] = True
            
        except Exception as e:
            print(f"TradeRecord CRUD í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # AgentPerformance CRUD í…ŒìŠ¤íŠ¸
        try:
            if self.test_data_ids['trade_records']:
                # Create
                agent_perf = self.create_sample_agent_performance("technical_analyst", str(self.test_data_ids['trade_records'][0]))
                perf_id = self.db_manager.add_agent_performance(agent_perf)
                self.test_data_ids['agent_performances'].append(perf_id)
                test_results['agent_performances']['create'] = True
                
                # Read
                retrieved_perf = self.db_manager.get_agent_performance(perf_id)
                test_results['agent_performances']['read'] = (retrieved_perf is not None)
                
                # Update (ì„±ê³¼ ì ìˆ˜ ì—…ë°ì´íŠ¸)
                if retrieved_perf:
                    updated = self.db_manager.update_agent_performance_score(perf_id, 0.95)
                    test_results['agent_performances']['update'] = updated
                
                test_results['agent_performances']['delete'] = True
                
        except Exception as e:
            print(f"AgentPerformance CRUD í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # LLMUsageLog CRUD í…ŒìŠ¤íŠ¸
        try:
            # Create
            usage_log = self.create_sample_llm_usage_log("cio")
            log_id = self.db_manager.add_llm_usage_log(usage_log)
            self.test_data_ids['llm_usage_logs'].append(log_id)
            test_results['llm_usage_logs']['create'] = True
            
            # Read
            retrieved_logs = self.db_manager.get_llm_usage_logs(session_id=self.test_session_id)
            test_results['llm_usage_logs']['read'] = (len(retrieved_logs) > 0)
            
            # UpdateëŠ” ì‚¬ìš©ëŸ‰ ë¡œê·¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
            test_results['llm_usage_logs']['update'] = True
            test_results['llm_usage_logs']['delete'] = True
            
        except Exception as e:
            print(f"LLMUsageLog CRUD í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # ModelEvolutionHistory CRUD í…ŒìŠ¤íŠ¸
        try:
            # Create
            model_evolution = self.create_sample_model_evolution("technical_analyst")
            evolution_id = self.db_manager.add_model_evolution_history(model_evolution)
            self.test_data_ids['model_evolution_histories'].append(evolution_id)
            test_results['model_evolution_histories']['create'] = True
            
            # Read
            retrieved_evolution = self.db_manager.get_model_evolution_history("technical_analyst")
            test_results['model_evolution_histories']['read'] = (len(retrieved_evolution) > 0)
            
            # UpdateëŠ” ì§„í™” íˆìŠ¤í† ë¦¬ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
            test_results['model_evolution_histories']['update'] = True
            test_results['model_evolution_histories']['delete'] = True
            
        except Exception as e:
            print(f"ModelEvolutionHistory CRUD í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
        total_operations = 0
        successful_operations = 0
        
        for table, operations in test_results.items():
            for operation, success in operations.items():
                total_operations += 1
                if success:
                    successful_operations += 1
        
        return {
            'test_type': 'basic_crud_operations',
            'total_operations': total_operations,
            'successful_operations': successful_operations,
            'success_rate': successful_operations / total_operations if total_operations > 0 else 0,
            'detailed_results': test_results
        }
    
    def test_agent_contribution_tracking(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        try:
            # ê±°ë˜ ê¸°ë¡ ìƒì„±
            trade_record = self.create_sample_trade_record()
            trade_id = self.db_manager.add_trade_record(trade_record)
            self.test_data_ids['trade_records'].append(trade_id)
            
            # ê° ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ê¸°ë¡ ìƒì„±
            agents = ["valuation_analyst", "technical_analyst", "risk_analyst", "cio"]
            contribution_weights = [0.3, 0.4, 0.2, 0.1]
            
            for agent, weight in zip(agents, contribution_weights):
                agent_perf = self.create_sample_agent_performance(agent, str(trade_id))
                agent_perf.contribution_weight = weight
                perf_id = self.db_manager.add_agent_performance(agent_perf)
                self.test_data_ids['agent_performances'].append(perf_id)
            
            # ê¸°ì—¬ë„ ê³„ì‚° ë° ê²€ì¦
            total_contribution = sum(contribution_weights)
            contribution_accuracy = abs(total_contribution - 1.0) < 0.01  # 1% ì˜¤ì°¨ í—ˆìš©
            
            # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ì¡°íšŒ
            agent_performances = []
            for agent in agents:
                perf_records = self.db_manager.get_agent_performance_by_type(agent)
                if perf_records:
                    agent_performances.append({
                        'agent_type': agent,
                        'performance_records': len(perf_records),
                        'latest_score': perf_records[-1].decision_quality_score if perf_records else 0
                    })
            
            return {
                'test_type': 'agent_contribution_tracking',
                'trade_created': True,
                'agents_tracked': len(agents),
                'contribution_accuracy': contribution_accuracy,
                'total_contribution': total_contribution,
                'agent_performances': agent_performances,
                'success': contribution_accuracy and len(agent_performances) == len(agents)
            }
            
        except Exception as e:
            return {
                'test_type': 'agent_contribution_tracking',
                'success': False,
                'error': str(e)
            }
    
    def test_llm_usage_aggregation(self) -> Dict[str, Any]:
        """LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ í…ŒìŠ¤íŠ¸"""
        try:
            # ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ì˜ ì‚¬ìš©ëŸ‰ ë¡œê·¸ ìƒì„±
            agents = ["cio", "technical_analyst", "valuation_analyst", "sector_researcher"]
            providers = ["claude", "openai", "google", "perplexity"]
            
            total_tokens = 0
            total_cost = 0.0
            
            for i, (agent, provider) in enumerate(zip(agents, providers)):
                usage_log = self.create_sample_llm_usage_log(agent)
                usage_log.provider = provider
                usage_log.tokens_used = 1000 + (i * 500)  # ë‹¤ì–‘í•œ í† í° ì‚¬ìš©ëŸ‰
                usage_log.cost = 0.02 + (i * 0.01)  # ë‹¤ì–‘í•œ ë¹„ìš©
                
                total_tokens += usage_log.tokens_used
                total_cost += usage_log.cost
                
                log_id = self.db_manager.add_llm_usage_log(usage_log)
                self.test_data_ids['llm_usage_logs'].append(log_id)
            
            # ì„¸ì…˜ë³„ ì‚¬ìš©ëŸ‰ ì§‘ê³„ ì¡°íšŒ
            session_logs = self.db_manager.get_llm_usage_logs(session_id=self.test_session_id)
            
            # ì§‘ê³„ ê²€ì¦
            retrieved_total_tokens = sum(log.tokens_used for log in session_logs)
            retrieved_total_cost = sum(log.cost for log in session_logs)
            
            tokens_accuracy = abs(retrieved_total_tokens - total_tokens) <= 10
            cost_accuracy = abs(retrieved_total_cost - total_cost) <= 0.01
            
            # ì œê³µì‚¬ë³„ ì§‘ê³„
            provider_stats = {}
            for log in session_logs:
                if log.provider not in provider_stats:
                    provider_stats[log.provider] = {'tokens': 0, 'cost': 0.0, 'calls': 0}
                provider_stats[log.provider]['tokens'] += log.tokens_used
                provider_stats[log.provider]['cost'] += log.cost
                provider_stats[log.provider]['calls'] += 1
            
            return {
                'test_type': 'llm_usage_aggregation',
                'logs_created': len(agents),
                'logs_retrieved': len(session_logs),
                'expected_tokens': total_tokens,
                'retrieved_tokens': retrieved_total_tokens,
                'expected_cost': total_cost,
                'retrieved_cost': retrieved_total_cost,
                'tokens_accuracy': tokens_accuracy,
                'cost_accuracy': cost_accuracy,
                'provider_stats': provider_stats,
                'success': tokens_accuracy and cost_accuracy and len(session_logs) == len(agents)
            }
            
        except Exception as e:
            return {
                'test_type': 'llm_usage_aggregation',
                'success': False,
                'error': str(e)
            }
    
    def test_model_evolution_tracking(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì§„í™” ì¶”ì  í…ŒìŠ¤íŠ¸"""
        try:
            # ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ëª¨ë¸ ì§„í™” íˆìŠ¤í† ë¦¬ ìƒì„±
            evolution_data = [
                ("technical_analyst", "gpt-4", "gpt-5", "Better technical analysis", 0.15),
                ("valuation_analyst", "gemini-flash", "gemini-flash-2.5", "Improved valuation accuracy", 0.12),
                ("cio", "claude-opus-3", "claude-opus-4.1", "Enhanced decision making", 0.20)
            ]
            
            for agent, old_model, new_model, reason, improvement in evolution_data:
                evolution = ModelEvolutionHistory(
                    agent_type=agent,
                    old_model=old_model,
                    new_model=new_model,
                    change_reason=reason,
                    performance_improvement=improvement,
                    timestamp=datetime.now()
                )
                
                evolution_id = self.db_manager.add_model_evolution_history(evolution)
                self.test_data_ids['model_evolution_histories'].append(evolution_id)
            
            # ì§„í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ë° ê²€ì¦
            all_evolutions = []
            for agent, _, _, _, _ in evolution_data:
                agent_evolutions = self.db_manager.get_model_evolution_history(agent)
                all_evolutions.extend(agent_evolutions)
            
            # ê²€ì¦
            expected_evolutions = len(evolution_data)
            retrieved_evolutions = len(all_evolutions)
            
            # ì„±ëŠ¥ ê°œì„  í‰ê·  ê³„ì‚°
            if all_evolutions:
                avg_improvement = sum(e.performance_improvement for e in all_evolutions) / len(all_evolutions)
            else:
                avg_improvement = 0
            
            return {
                'test_type': 'model_evolution_tracking',
                'evolutions_created': expected_evolutions,
                'evolutions_retrieved': retrieved_evolutions,
                'avg_performance_improvement': avg_improvement,
                'evolution_details': [
                    {
                        'agent_type': e.agent_type,
                        'model_change': f"{e.old_model} -> {e.new_model}",
                        'improvement': e.performance_improvement
                    } for e in all_evolutions
                ],
                'success': retrieved_evolutions >= expected_evolutions
            }
            
        except Exception as e:
            return {
                'test_type': 'model_evolution_tracking',
                'success': False,
                'error': str(e)
            }
    
    def test_data_integrity_and_relationships(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ë° ê´€ê³„ í…ŒìŠ¤íŠ¸"""
        try:
            # ê±°ë˜ ê¸°ë¡ ìƒì„±
            trade_record = self.create_sample_trade_record()
            trade_id = self.db_manager.add_trade_record(trade_record)
            
            # ê´€ë ¨ ì—ì´ì „íŠ¸ ì„±ê³¼ ê¸°ë¡ ìƒì„±
            agent_perf = self.create_sample_agent_performance("technical_analyst", str(trade_id))
            perf_id = self.db_manager.add_agent_performance(agent_perf)
            
            # ê´€ë ¨ LLM ì‚¬ìš©ëŸ‰ ë¡œê·¸ ìƒì„±
            usage_log = self.create_sample_llm_usage_log("technical_analyst")
            log_id = self.db_manager.add_llm_usage_log(usage_log)
            
            # ë°ì´í„° ê´€ê³„ ê²€ì¦
            # 1. ê±°ë˜ ê¸°ë¡ ì¡°íšŒ
            retrieved_trade = self.db_manager.get_trade_record(trade_id)
            trade_exists = retrieved_trade is not None
            
            # 2. í•´ë‹¹ ê±°ë˜ì˜ ì—ì´ì „íŠ¸ ì„±ê³¼ ì¡°íšŒ
            agent_performances = self.db_manager.get_agent_performance_by_trade(str(trade_id))
            performance_linked = len(agent_performances) > 0
            
            # 3. ì„¸ì…˜ ì‚¬ìš©ëŸ‰ ë¡œê·¸ ì¡°íšŒ
            session_logs = self.db_manager.get_llm_usage_logs(session_id=self.test_session_id)
            usage_logged = len(session_logs) > 0
            
            # 4. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
            if retrieved_trade and agent_performances:
                agent_contribution = agent_performances[0].contribution_weight
                trade_contribution = retrieved_trade.agent_contributions.get("technical_analyst", 0)
                contribution_consistent = abs(agent_contribution - trade_contribution) < 0.1
            else:
                contribution_consistent = False
            
            # ì„ì‹œ ì €ì¥ (ì •ë¦¬ìš©)
            self.test_data_ids['trade_records'].append(trade_id)
            self.test_data_ids['agent_performances'].append(perf_id)
            self.test_data_ids['llm_usage_logs'].append(log_id)
            
            return {
                'test_type': 'data_integrity_relationships',
                'trade_exists': trade_exists,
                'performance_linked': performance_linked,
                'usage_logged': usage_logged,
                'contribution_consistent': contribution_consistent,
                'all_relationships_valid': all([trade_exists, performance_linked, usage_logged, contribution_consistent]),
                'success': all([trade_exists, performance_linked, usage_logged])
            }
            
        except Exception as e:
            return {
                'test_type': 'data_integrity_relationships',
                'success': False,
                'error': str(e)
            }
    
    def cleanup_test_data(self) -> None:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬"""
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì•¼ í•˜ì§€ë§Œ,
        # ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” ë””ë²„ê¹…ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ìœ ì§€
        print(f"í…ŒìŠ¤íŠ¸ ì„¸ì…˜ {self.test_session_id} ë°ì´í„° ìœ ì§€ë¨ (ë””ë²„ê¹…ìš©)")
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {}
        
        try:
            # 1. ê¸°ë³¸ CRUD ì‘ì—… í…ŒìŠ¤íŠ¸
            print("ğŸ“ ê¸°ë³¸ CRUD ì‘ì—… í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_results['crud_operations'] = self.test_basic_crud_operations()
            
            # 2. ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  í…ŒìŠ¤íŠ¸
            print("ğŸ¤– ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_results['contribution_tracking'] = self.test_agent_contribution_tracking()
            
            # 3. LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ í…ŒìŠ¤íŠ¸
            print("ğŸ“Š LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_results['usage_aggregation'] = self.test_llm_usage_aggregation()
            
            # 4. ëª¨ë¸ ì§„í™” ì¶”ì  í…ŒìŠ¤íŠ¸
            print("ğŸ§¬ ëª¨ë¸ ì§„í™” ì¶”ì  í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_results['evolution_tracking'] = self.test_model_evolution_tracking()
            
            # 5. ë°ì´í„° ë¬´ê²°ì„± ë° ê´€ê³„ í…ŒìŠ¤íŠ¸
            print("ğŸ”— ë°ì´í„° ë¬´ê²°ì„± ë° ê´€ê³„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_results['data_integrity'] = self.test_data_integrity_and_relationships()
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            all_tests_passed = all([
                test_results['crud_operations']['success_rate'] >= 0.9,
                test_results['contribution_tracking']['success'],
                test_results['usage_aggregation']['success'],
                test_results['evolution_tracking']['success'],
                test_results['data_integrity']['success']
            ])
            
            test_results['summary'] = {
                'crud_operations_passed': test_results['crud_operations']['success_rate'] >= 0.9,
                'contribution_tracking_passed': test_results['contribution_tracking']['success'],
                'usage_aggregation_passed': test_results['usage_aggregation']['success'],
                'evolution_tracking_passed': test_results['evolution_tracking']['success'],
                'data_integrity_passed': test_results['data_integrity']['success'],
                'all_tests_passed': all_tests_passed
            }
            
        finally:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
            self.cleanup_test_data()
        
        return test_results


# pytest í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
class TestDatabaseIntegration:
    """pytest ê¸°ë°˜ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def tester(self):
        """í…ŒìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return DatabaseIntegrationTester()
    
    def test_crud_operations(self, tester):
        """CRUD ì‘ì—… í…ŒìŠ¤íŠ¸"""
        result = tester.test_basic_crud_operations()
        
        assert result['success_rate'] >= 0.9, f"CRUD success rate {result['success_rate']:.1%} is below 90%"
        
        print(f"âœ… CRUD ì‘ì—… í…ŒìŠ¤íŠ¸ í†µê³¼: {result['success_rate']:.1%}")
    
    def test_agent_contribution_tracking(self, tester):
        """ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        result = tester.test_agent_contribution_tracking()
        
        assert result['success'], f"Agent contribution tracking failed: {result.get('error', 'Unknown error')}"
        
        print(f"âœ… ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_llm_usage_aggregation(self, tester):
        """LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ í…ŒìŠ¤íŠ¸"""
        result = tester.test_llm_usage_aggregation()
        
        assert result['success'], f"LLM usage aggregation failed: {result.get('error', 'Unknown error')}"
        
        print(f"âœ… LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_model_evolution_tracking(self, tester):
        """ëª¨ë¸ ì§„í™” ì¶”ì  í…ŒìŠ¤íŠ¸"""
        result = tester.test_model_evolution_tracking()
        
        assert result['success'], f"Model evolution tracking failed: {result.get('error', 'Unknown error')}"
        
        print(f"âœ… ëª¨ë¸ ì§„í™” ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_data_integrity(self, tester):
        """ë°ì´í„° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸"""
        result = tester.test_data_integrity_and_relationships()
        
        assert result['success'], f"Data integrity test failed: {result.get('error', 'Unknown error')}"
        
        print(f"âœ… ë°ì´í„° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = DatabaseIntegrationTester()
    results = tester.run_comprehensive_test()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    
    # CRUD ì‘ì—… ê²°ê³¼
    crud_result = results['crud_operations']
    print(f"\nğŸ“ ê¸°ë³¸ CRUD ì‘ì—…:")
    print(f"   ì„±ê³µë¥ : {crud_result['success_rate']:.1%} ({crud_result['successful_operations']}/{crud_result['total_operations']})")
    
    # ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì  ê²°ê³¼
    contribution_result = results['contribution_tracking']
    print(f"\nğŸ¤– ì—ì´ì „íŠ¸ ê¸°ì—¬ë„ ì¶”ì :")
    print(f"   ì„±ê³µ: {'âœ…' if contribution_result['success'] else 'âŒ'}")
    if contribution_result['success']:
        print(f"   ì¶”ì ëœ ì—ì´ì „íŠ¸: {contribution_result['agents_tracked']}")
        print(f"   ê¸°ì—¬ë„ ì •í™•ì„±: {'âœ…' if contribution_result['contribution_accuracy'] else 'âŒ'}")
    
    # LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„ ê²°ê³¼
    usage_result = results['usage_aggregation']
    print(f"\nğŸ“Š LLM ì‚¬ìš©ëŸ‰ ì§‘ê³„:")
    print(f"   ì„±ê³µ: {'âœ…' if usage_result['success'] else 'âŒ'}")
    if usage_result['success']:
        print(f"   ë¡œê·¸ ê¸°ë¡: {usage_result['logs_created']}")
        print(f"   í† í° ì •í™•ì„±: {'âœ…' if usage_result['tokens_accuracy'] else 'âŒ'}")
        print(f"   ë¹„ìš© ì •í™•ì„±: {'âœ…' if usage_result['cost_accuracy'] else 'âŒ'}")
    
    # ëª¨ë¸ ì§„í™” ì¶”ì  ê²°ê³¼
    evolution_result = results['evolution_tracking']
    print(f"\nğŸ§¬ ëª¨ë¸ ì§„í™” ì¶”ì :")
    print(f"   ì„±ê³µ: {'âœ…' if evolution_result['success'] else 'âŒ'}")
    if evolution_result['success']:
        print(f"   ì§„í™” ê¸°ë¡: {evolution_result['evolutions_created']}")
        print(f"   í‰ê·  ì„±ëŠ¥ ê°œì„ : {evolution_result['avg_performance_improvement']:.1%}")
    
    # ë°ì´í„° ë¬´ê²°ì„± ê²°ê³¼
    integrity_result = results['data_integrity']
    print(f"\nğŸ”— ë°ì´í„° ë¬´ê²°ì„±:")
    print(f"   ì„±ê³µ: {'âœ…' if integrity_result['success'] else 'âŒ'}")
    if integrity_result['success']:
        print(f"   ê±°ë˜ ë°ì´í„°: {'âœ…' if integrity_result['trade_exists'] else 'âŒ'}")
        print(f"   ì„±ê³¼ ì—°ê²°: {'âœ…' if integrity_result['performance_linked'] else 'âŒ'}")
        print(f"   ì‚¬ìš©ëŸ‰ ë¡œê¹…: {'âœ…' if integrity_result['usage_logged'] else 'âŒ'}")
    
    # ì „ì²´ ìš”ì•½
    summary = results['summary']
    print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
    print(f"   CRUD ì‘ì—…: {'âœ… PASS' if summary['crud_operations_passed'] else 'âŒ FAIL'}")
    print(f"   ê¸°ì—¬ë„ ì¶”ì : {'âœ… PASS' if summary['contribution_tracking_passed'] else 'âŒ FAIL'}")
    print(f"   ì‚¬ìš©ëŸ‰ ì§‘ê³„: {'âœ… PASS' if summary['usage_aggregation_passed'] else 'âŒ FAIL'}")
    print(f"   ì§„í™” ì¶”ì : {'âœ… PASS' if summary['evolution_tracking_passed'] else 'âŒ FAIL'}")
    print(f"   ë°ì´í„° ë¬´ê²°ì„±: {'âœ… PASS' if summary['data_integrity_passed'] else 'âŒ FAIL'}")
    print(f"   ì „ì²´ í…ŒìŠ¤íŠ¸: {'âœ… PASS' if summary['all_tests_passed'] else 'âŒ FAIL'}")
    
    return results


if __name__ == "__main__":
    main()