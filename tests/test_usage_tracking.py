#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
API ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹œìŠ¤í…œ ê²€ì¦ í…ŒìŠ¤íŠ¸

LLM ì‚¬ìš©ëŸ‰ ë¡œê¹…, ë¹„ìš© ê³„ì‚°, ëŒ€ì‹œë³´ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import time
from datetime import datetime, timedelta
from typing import Dict, List, Any

# pytestëŠ” ì„ íƒì ìœ¼ë¡œ import
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False

from src.llm_clients.client_factory import LLMClientFactory
from src.llm_clients.base import LLMRequest, UsageStats
from src.database.schema import DatabaseManager, LLMUsageLog, AgentPerformance


class UsageTrackingTester:
    """ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.client_factory = LLMClientFactory()
        self.db_manager = DatabaseManager()
        self.test_session_id = f"test_session_{int(time.time())}"
    
    def create_test_request(self, agent_type: str, size: str = "small") -> LLMRequest:
        """ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ŒìŠ¤íŠ¸ ìš”ì²­ ìƒì„±"""
        prompts = {
            "small": "Analyze AAPL stock briefly.",
            "medium": "Provide a detailed analysis of AAPL stock including fundamentals, technicals, and market sentiment. " * 10,
            "large": "Conduct a comprehensive investment analysis covering all aspects of Apple Inc. " * 50
        }
        
        max_tokens = {
            "small": 200,
            "medium": 1000, 
            "large": 2000
        }
        
        return LLMRequest(
            prompt=prompts.get(size, prompts["small"]),
            agent_type=agent_type,
            max_tokens=max_tokens.get(size, 200),
            temperature=0.1
        )
    
    def test_token_counting_accuracy(self) -> Dict[str, Any]:
        """í† í° ì¹´ìš´íŒ… ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        test_results = []
        
        test_cases = [
            ("valuation_analyst", "small"),
            ("technical_analyst", "medium"),
            ("cio", "large")
        ]
        
        for agent_type, size in test_cases:
            try:
                client = self.client_factory.get_client(agent_type)
                request = self.create_test_request(agent_type, size)
                
                # ì‚¬ìš©ëŸ‰ í†µê³„ ì´ˆê¸°í™”
                initial_stats = client.get_usage_stats()
                initial_tokens = initial_stats.total_tokens_used
                
                # ìš”ì²­ ì‹¤í–‰
                response = client.generate_response(request)
                
                # ì‚¬ìš©ëŸ‰ í†µê³„ í™•ì¸
                final_stats = client.get_usage_stats()
                token_increase = final_stats.total_tokens_used - initial_tokens
                
                # í† í° ìˆ˜ ê²€ì¦ (ì‘ë‹µì—ì„œ ë³´ê³ ëœ í† í°ê³¼ í†µê³„ì˜ ì¦ê°€ëŸ‰ì´ ì¼ì¹˜í•´ì•¼ í•¨)
                token_accuracy = abs(token_increase - response.tokens_used) <= 5  # 5 í† í° ì˜¤ì°¨ í—ˆìš©
                
                test_results.append({
                    'agent_type': agent_type,
                    'request_size': size,
                    'reported_tokens': response.tokens_used,
                    'stats_increase': token_increase,
                    'token_accuracy': token_accuracy,
                    'cost': response.cost,
                    'model_used': response.model_used,
                    'success': True
                })
                
            except Exception as e:
                test_results.append({
                    'agent_type': agent_type,
                    'request_size': size,
                    'success': False,
                    'error': str(e)
                })
        
        # ì •í™•ì„± ê³„ì‚°
        successful_tests = [r for r in test_results if r.get('success', False)]
        accurate_tests = [r for r in successful_tests if r.get('token_accuracy', False)]
        
        accuracy_rate = len(accurate_tests) / len(successful_tests) if successful_tests else 0
        
        return {
            'test_type': 'token_counting_accuracy',
            'total_tests': len(test_cases),
            'successful_tests': len(successful_tests),
            'accurate_tests': len(accurate_tests),
            'accuracy_rate': accuracy_rate,
            'individual_results': test_results
        }
    
    def test_cost_calculation_accuracy(self) -> Dict[str, Any]:
        """ë¹„ìš© ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        test_results = []
        
        # ê° LLM ì œê³µì‚¬ë³„ ë¹„ìš© ê³„ì‚° í…ŒìŠ¤íŠ¸
        provider_agents = {
            'claude': 'cio',
            'gpt': 'technical_analyst',
            'gemini': 'valuation_analyst',
            'perplexity': 'sector_researcher'
        }
        
        for provider, agent_type in provider_agents.items():
            try:
                client = self.client_factory.get_client(agent_type)
                request = self.create_test_request(agent_type, "medium")
                
                # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
                estimated_cost = client.estimate_cost(request)
                
                # ì‹¤ì œ ìš”ì²­ ì‹¤í–‰
                response = client.generate_response(request)
                actual_cost = response.cost
                
                # ë¹„ìš© ì •í™•ì„± ê²€ì¦ (20% ì˜¤ì°¨ í—ˆìš©)
                cost_difference = abs(estimated_cost - actual_cost)
                cost_accuracy = cost_difference <= (estimated_cost * 0.2)
                
                test_results.append({
                    'provider': provider,
                    'agent_type': agent_type,
                    'estimated_cost': estimated_cost,
                    'actual_cost': actual_cost,
                    'cost_difference': cost_difference,
                    'cost_accuracy': cost_accuracy,
                    'tokens_used': response.tokens_used,
                    'success': True
                })
                
            except Exception as e:
                test_results.append({
                    'provider': provider,
                    'agent_type': agent_type,
                    'success': False,
                    'error': str(e)
                })
        
        successful_tests = [r for r in test_results if r.get('success', False)]
        accurate_tests = [r for r in successful_tests if r.get('cost_accuracy', False)]
        
        accuracy_rate = len(accurate_tests) / len(successful_tests) if successful_tests else 0
        
        return {
            'test_type': 'cost_calculation_accuracy',
            'total_providers': len(provider_agents),
            'successful_tests': len(successful_tests),
            'accurate_tests': len(accurate_tests),
            'accuracy_rate': accuracy_rate,
            'individual_results': test_results
        }
    
    def test_usage_logging_to_database(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ëŸ‰ ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        test_agents = ['cio', 'technical_analyst', 'valuation_analyst']
        logged_records = []
        
        for agent_type in test_agents:
            try:
                client = self.client_factory.get_client(agent_type)
                request = self.create_test_request(agent_type, "small")
                
                # ìš”ì²­ ì‹¤í–‰
                response = client.generate_response(request)
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡ (ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” ìë™)
                usage_log = LLMUsageLog(
                    session_id=self.test_session_id,
                    agent_type=agent_type,
                    provider=response.model_used.split('-')[0] if response.model_used else 'unknown',
                    model_name=response.model_used,
                    tokens_used=response.tokens_used,
                    cost=response.cost,
                    response_time=response.response_time,
                    timestamp=datetime.now()
                )
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                log_id = self.db_manager.add_llm_usage_log(usage_log)
                
                logged_records.append({
                    'log_id': log_id,
                    'agent_type': agent_type,
                    'tokens_used': response.tokens_used,
                    'cost': response.cost,
                    'model_used': response.model_used,
                    'success': True
                })
                
            except Exception as e:
                logged_records.append({
                    'agent_type': agent_type,
                    'success': False,
                    'error': str(e)
                })
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œê·¸ ì¡°íšŒí•˜ì—¬ ê²€ì¦
        try:
            retrieved_logs = self.db_manager.get_llm_usage_logs(session_id=self.test_session_id)
            retrieved_count = len(retrieved_logs)
            expected_count = len([r for r in logged_records if r['success']])
            
            logging_accuracy = retrieved_count == expected_count
            
        except Exception as e:
            retrieved_count = 0
            logging_accuracy = False
        
        successful_logs = [r for r in logged_records if r['success']]
        
        return {
            'test_type': 'usage_logging',
            'total_attempts': len(test_agents),
            'successful_logs': len(successful_logs),
            'retrieved_logs': retrieved_count,
            'logging_accuracy': logging_accuracy,
            'individual_results': logged_records
        }
    
    def test_real_time_stats_tracking(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ í†µê³„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        agent_type = 'cio'
        client = self.client_factory.get_client(agent_type)
        
        # ì´ˆê¸° ìƒíƒœ ê¸°ë¡
        initial_stats = client.get_usage_stats()
        
        # ì—¬ëŸ¬ ìš”ì²­ ì‹¤í–‰
        num_requests = 3
        requests_made = 0
        successful_requests = 0
        total_tokens = 0
        total_cost = 0.0
        
        for i in range(num_requests):
            try:
                request = self.create_test_request(agent_type, "small")
                response = client.generate_response(request)
                
                requests_made += 1
                successful_requests += 1
                total_tokens += response.tokens_used
                total_cost += response.cost
                
            except Exception:
                requests_made += 1
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        final_stats = client.get_usage_stats()
        
        # í†µê³„ ì •í™•ì„± ê²€ì¦
        expected_total_requests = initial_stats.total_requests + requests_made
        expected_successful_requests = initial_stats.successful_requests + successful_requests
        expected_total_tokens = initial_stats.total_tokens_used + total_tokens
        expected_total_cost = initial_stats.total_cost + total_cost
        
        stats_accuracy = (
            final_stats.total_requests == expected_total_requests and
            final_stats.successful_requests == expected_successful_requests and
            abs(final_stats.total_tokens_used - expected_total_tokens) <= 10 and  # 10 í† í° ì˜¤ì°¨ í—ˆìš©
            abs(final_stats.total_cost - expected_total_cost) <= 0.01  # $0.01 ì˜¤ì°¨ í—ˆìš©
        )
        
        return {
            'test_type': 'real_time_stats',
            'requests_made': requests_made,
            'successful_requests': successful_requests,
            'initial_stats': {
                'total_requests': initial_stats.total_requests,
                'successful_requests': initial_stats.successful_requests,
                'total_tokens': initial_stats.total_tokens_used,
                'total_cost': initial_stats.total_cost
            },
            'final_stats': {
                'total_requests': final_stats.total_requests,
                'successful_requests': final_stats.successful_requests,
                'total_tokens': final_stats.total_tokens_used,
                'total_cost': final_stats.total_cost
            },
            'expected_stats': {
                'total_requests': expected_total_requests,
                'successful_requests': expected_successful_requests,
                'total_tokens': expected_total_tokens,
                'total_cost': expected_total_cost
            },
            'stats_accuracy': stats_accuracy
        }
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ì‚¬ìš©ëŸ‰ ì¶”ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ API ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {}
        
        # 1. í† í° ì¹´ìš´íŒ… ì •í™•ì„± í…ŒìŠ¤íŠ¸
        print("ğŸ”¢ í† í° ì¹´ìš´íŒ… ì •í™•ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['token_counting'] = self.test_token_counting_accuracy()
        
        # 2. ë¹„ìš© ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸
        print("ğŸ’° ë¹„ìš© ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['cost_calculation'] = self.test_cost_calculation_accuracy()
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹… í…ŒìŠ¤íŠ¸
        print("ğŸ“ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ëŸ‰ ë¡œê¹… í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['usage_logging'] = self.test_usage_logging_to_database()
        
        # 4. ì‹¤ì‹œê°„ í†µê³„ ì¶”ì  í…ŒìŠ¤íŠ¸
        print("ğŸ“Š ì‹¤ì‹œê°„ í†µê³„ ì¶”ì  í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_results['real_time_stats'] = self.test_real_time_stats_tracking()
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        all_tests_passed = all([
            test_results['token_counting']['accuracy_rate'] >= 0.9,
            test_results['cost_calculation']['accuracy_rate'] >= 0.8,
            test_results['usage_logging']['logging_accuracy'],
            test_results['real_time_stats']['stats_accuracy']
        ])
        
        test_results['summary'] = {
            'token_counting_passed': test_results['token_counting']['accuracy_rate'] >= 0.9,
            'cost_calculation_passed': test_results['cost_calculation']['accuracy_rate'] >= 0.8,
            'usage_logging_passed': test_results['usage_logging']['logging_accuracy'],
            'real_time_stats_passed': test_results['real_time_stats']['stats_accuracy'],
            'all_tests_passed': all_tests_passed
        }
        
        return test_results


# pytest í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
class TestUsageTracking:
    """pytest ê¸°ë°˜ ì‚¬ìš©ëŸ‰ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def tester(self):
        """í…ŒìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return UsageTrackingTester()
    
    def test_token_counting_accuracy(self, tester):
        """í† í° ì¹´ìš´íŒ… ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        result = tester.test_token_counting_accuracy()
        
        assert result['accuracy_rate'] >= 0.9, f"Token counting accuracy {result['accuracy_rate']:.1%} is below 90%"
        
        print(f"âœ… í† í° ì¹´ìš´íŒ… ì •í™•ì„± í…ŒìŠ¤íŠ¸ í†µê³¼: {result['accuracy_rate']:.1%}")
    
    def test_cost_calculation_accuracy(self, tester):
        """ë¹„ìš© ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        result = tester.test_cost_calculation_accuracy()
        
        assert result['accuracy_rate'] >= 0.8, f"Cost calculation accuracy {result['accuracy_rate']:.1%} is below 80%"
        
        print(f"âœ… ë¹„ìš© ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸ í†µê³¼: {result['accuracy_rate']:.1%}")
    
    def test_database_logging(self, tester):
        """ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        result = tester.test_usage_logging_to_database()
        
        assert result['logging_accuracy'], "Database logging failed"
        
        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹… í…ŒìŠ¤íŠ¸ í†µê³¼: {result['successful_logs']}/{result['total_attempts']}")
    
    def test_real_time_stats(self, tester):
        """ì‹¤ì‹œê°„ í†µê³„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        result = tester.test_real_time_stats_tracking()
        
        assert result['stats_accuracy'], "Real-time stats tracking failed"
        
        print(f"âœ… ì‹¤ì‹œê°„ í†µê³„ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = UsageTrackingTester()
    results = tester.run_comprehensive_test()
    
    print("\n" + "="*60)
    print("ğŸ“‹ API ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    
    # í† í° ì¹´ìš´íŒ… ê²°ê³¼
    token_result = results['token_counting']
    print(f"\nğŸ”¢ í† í° ì¹´ìš´íŒ… ì •í™•ì„±:")
    print(f"   ì •í™•ë„: {token_result['accuracy_rate']:.1%} ({token_result['accurate_tests']}/{token_result['successful_tests']})")
    
    # ë¹„ìš© ê³„ì‚° ê²°ê³¼
    cost_result = results['cost_calculation']
    print(f"\nğŸ’° ë¹„ìš© ê³„ì‚° ì •í™•ì„±:")
    print(f"   ì •í™•ë„: {cost_result['accuracy_rate']:.1%} ({cost_result['accurate_tests']}/{cost_result['successful_tests']})")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹… ê²°ê³¼
    logging_result = results['usage_logging']
    print(f"\nğŸ“ ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹…:")
    print(f"   ì„±ê³µ: {'âœ…' if logging_result['logging_accuracy'] else 'âŒ'}")
    print(f"   ë¡œê·¸ ê¸°ë¡: {logging_result['successful_logs']}/{logging_result['total_attempts']}")
    print(f"   ì¡°íšŒ ê²€ì¦: {logging_result['retrieved_logs']}ê°œ ë¡œê·¸ ì¡°íšŒë¨")
    
    # ì‹¤ì‹œê°„ í†µê³„ ê²°ê³¼
    stats_result = results['real_time_stats']
    print(f"\nğŸ“Š ì‹¤ì‹œê°„ í†µê³„ ì¶”ì :")
    print(f"   ì •í™•ì„±: {'âœ…' if stats_result['stats_accuracy'] else 'âŒ'}")
    print(f"   ìš”ì²­ ìˆ˜í–‰: {stats_result['successful_requests']}/{stats_result['requests_made']}")
    
    # ì „ì²´ ìš”ì•½
    summary = results['summary']
    print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
    print(f"   í† í° ì¹´ìš´íŒ…: {'âœ… PASS' if summary['token_counting_passed'] else 'âŒ FAIL'}")
    print(f"   ë¹„ìš© ê³„ì‚°: {'âœ… PASS' if summary['cost_calculation_passed'] else 'âŒ FAIL'}")
    print(f"   ë°ì´í„°ë² ì´ìŠ¤ ë¡œê¹…: {'âœ… PASS' if summary['usage_logging_passed'] else 'âŒ FAIL'}")
    print(f"   ì‹¤ì‹œê°„ í†µê³„: {'âœ… PASS' if summary['real_time_stats_passed'] else 'âŒ FAIL'}")
    print(f"   ì „ì²´ í…ŒìŠ¤íŠ¸: {'âœ… PASS' if summary['all_tests_passed'] else 'âŒ FAIL'}")
    
    return results


if __name__ == "__main__":
    main()